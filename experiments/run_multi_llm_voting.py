"""
å¤šLLMåŠ æƒæŠ•ç¥¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ V2
æ”¯æŒåŒæ„/å¼‚æ„LLMé…ç½®ã€æ‰«ææ¨¡å¼ã€è¯¦ç»†æ•°æ®è®°å½•

Features:
- åŒæ„LLM: æŒ‡å®šå•ä¸ªLLMåå­—å’Œæ•°é‡
- å¼‚æ„LLM: æŒ‡å®šLLMåå­—åˆ—è¡¨
- æ‰«ææ¨¡å¼: è‡ªåŠ¨æµ‹è¯•1åˆ°Nä¸ªLLMçš„æŠ•ç¥¨ç»“æœ
- è¯¦ç»†æ•°æ®è®°å½•: ä¿å­˜æ¯é“é¢˜çš„æŠ•ç¥¨åˆ†å¸ƒã€ç½®ä¿¡åº¦ç­‰å…ƒæ•°æ®
- WandBé›†æˆ: å®æ—¶è®°å½•å’Œå¯è§†åŒ–

Usage:
    # åŒæ„LLM (5ä¸ªç›¸åŒæ¨¡å‹)
    python run_multi_llm_voting_v2.py --homogeneous --llm_name "Qwen/Qwen3-4B" --num_agents 5

    # å¼‚æ„LLM (æŒ‡å®šä¸åŒæ¨¡å‹åˆ—è¡¨)
    python run_multi_llm_voting_v2.py --heterogeneous --llm_names "Qwen/Qwen3-0.6B" "Qwen/Qwen3-1.7B" "Qwen/Qwen3-4B"

    # æ‰«ææ¨¡å¼ (æµ‹è¯•1åˆ°Nä¸ªLLM)
    python run_multi_llm_voting_v2.py --homogeneous --llm_name "Qwen/Qwen3-4B" --num_agents 10 --scan_mode
"""

import sys
import os

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
sys.stdout.reconfigure(encoding='utf-8')

import asyncio
import argparse
import time
import json
import pickle
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field, asdict
from collections import Counter, defaultdict
from tqdm import tqdm
import math

import weave

# å¯¼å…¥é¡¹ç›®ä¾èµ–
from GDesigner.llm.llm_registry import LLMRegistry
from GDesigner.utils.globals import Cost, PromptTokens, CompletionTokens
from GDesigner.utils.const import GDesigner_ROOT
from dataset.mmlu_dataset import MMLUDataset
from dataset.MMLU.download import download


# ============================================================================
# æ•°æ®ç±»å®šä¹‰ - ç”¨äºç»“æ„åŒ–å­˜å‚¨å®éªŒæ•°æ®
# ============================================================================

@dataclass
class AgentVote:
    """å•ä¸ªæ™ºèƒ½ä½“çš„æŠ•ç¥¨è®°å½•"""
    agent_id: str
    llm_name: str
    weight: float
    raw_response: str
    extracted_answer: str
    response_time: float
    is_correct: bool = False


@dataclass
class QuestionRecord:
    """å•é“é¢˜ç›®çš„å®Œæ•´è®°å½•"""
    question_id: int
    question_text: str
    correct_answer: str
    agent_votes: List[AgentVote] = field(default_factory=list)

    # æŠ•ç¥¨ç»Ÿè®¡
    final_answer: str = ""
    is_correct: bool = False
    vote_distribution: Dict[str, float] = field(default_factory=dict)  # answer -> weighted score
    raw_vote_counts: Dict[str, int] = field(default_factory=dict)  # answer -> count

    # ä¸€è‡´æ€§æŒ‡æ ‡
    is_unanimous: bool = False
    agreement_ratio: float = 0.0  # æœ€é«˜ç¥¨ç­”æ¡ˆçš„å æ¯”
    entropy: float = 0.0  # æŠ•ç¥¨åˆ†å¸ƒçš„ç†µ

    # æ—¶é—´
    total_time: float = 0.0


@dataclass
class ScanResult:
    """æ‰«ææ¨¡å¼ä¸‹æŸä¸ªagentæ•°é‡çš„ç»“æœ"""
    num_agents: int
    agent_ids: List[str]
    accuracy: float
    correct_count: int
    total_count: int
    unanimous_ratio: float
    avg_agreement_ratio: float
    avg_time: float


@dataclass
class ExperimentMetadata:
    """å®éªŒå…ƒæ•°æ®"""
    experiment_id: str
    timestamp: str
    config: Dict[str, Any]

    # LLMé…ç½®
    llm_configs: List[Tuple[str, float]]
    is_homogeneous: bool

    # æ•°æ®é›†ä¿¡æ¯
    dataset_name: str
    dataset_split: str
    total_questions: int

    # ç»“æœæ±‡æ€»
    question_records: List[QuestionRecord] = field(default_factory=list)
    scan_results: List[ScanResult] = field(default_factory=list)

    # æ€§èƒ½æŒ‡æ ‡
    total_cost: float = 0.0
    total_prompt_tokens: int = 0
    total_completion_tokens: int = 0
    total_time: float = 0.0


# ============================================================================
# æ ¸å¿ƒç±»
# ============================================================================

class VotingAgent:
    """å•ä¸ªæŠ•ç¥¨æ™ºèƒ½ä½“"""

    def __init__(self, agent_id: str, llm_name: str, weight: float = 1.0, temperature=0.7, enable_thinking=True):
        self.agent_id = agent_id
        self.llm_name = llm_name
        self.weight = weight
        self.llm = LLMRegistry.get(llm_name)
        self.temperature = temperature
        self.enable_thinking = enable_thinking

    async def vote(self, question: str) -> Tuple[str, float]:
        """å¯¹é—®é¢˜è¿›è¡ŒæŠ•ç¥¨ï¼Œè¿”å›(åŸå§‹å“åº”, å“åº”æ—¶é—´)"""

        system_prompt = """You are an expert at multiple-choice questions.
You will be given a question with 4 options (A, B, C, D).
Only one answer is correct.

IMPORTANT OUTPUT FORMAT:
1. You can use <think>...</think> tags to show your reasoning process
2. After your thinking, you MUST output your final answer in this EXACT format:
   **Answer: X**
   where X is one of A, B, C, or D

Example output format:
<think>
...
</think>

**Answer: B**"""

        user_prompt = f"{question}\n\nRemember: End your response with **Answer: X** where X is your chosen letter."

        messages = [
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': user_prompt}
        ]

        start_time = time.time()
        # response = await self.llm.agen(messages, temperature=self.temperature)
        response = await self.llm.acomp(messages, temperature=self.temperature, enable_thinking=self.enable_thinking)
        elapsed_time = time.time() - start_time

        return response, elapsed_time


class MultiLLMVotingSystemV2:
    """å¤šLLMåŠ æƒæŠ•ç¥¨ç³»ç»Ÿ V2"""

    def __init__(self, llm_configs: List[Tuple[str, float]], temperature=0.7, enable_thinking=True):
        """
        Args:
            llm_configs: List of (llm_name, weight) tuples
        """
        self.agents: List[VotingAgent] = []

        for idx, (llm_name, weight) in enumerate(llm_configs):
            agent_id = f"agent_{idx}_{llm_name.split('/')[-1]}"
            agent = VotingAgent(agent_id, llm_name, weight, temperature=temperature, enable_thinking=enable_thinking)
            self.agents.append(agent)

        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(agent.weight for agent in self.agents)
        if total_weight > 0:
            for agent in self.agents:
                agent.weight /= total_weight

    def get_subset(self, num_agents: int) -> 'MultiLLMVotingSystemV2':
        """è·å–å‰nä¸ªagentçš„å­é›†"""
        subset_configs = [(agent.llm_name, agent.weight) for agent in self.agents[:num_agents]]
        return MultiLLMVotingSystemV2(subset_configs)

    async def vote_on_question(
            self,
            question_id: int,
            question: str,
            correct_answer: str,
            active_agent_ids: Optional[List[str]] = None
    ) -> QuestionRecord:
        """
        å¯¹å•ä¸ªé—®é¢˜è¿›è¡ŒæŠ•ç¥¨

        Args:
            question_id: é—®é¢˜ID
            question: é—®é¢˜æ–‡æœ¬
            correct_answer: æ­£ç¡®ç­”æ¡ˆ
            active_agent_ids: å‚ä¸æŠ•ç¥¨çš„agent IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨å‚ä¸
        """
        record = QuestionRecord(
            question_id=question_id,
            question_text=question[:500],  # æˆªæ–­ä¿å­˜
            correct_answer=correct_answer
        )

        start_time = time.time()

        # ç¡®å®šå‚ä¸çš„agents
        active_agents = self.agents
        if active_agent_ids:
            active_agents = [a for a in self.agents if a.agent_id in active_agent_ids]

        # å¹¶å‘æ”¶é›†æŠ•ç¥¨
        tasks = [agent.vote(question) for agent in active_agents]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        vote_scores = defaultdict(float)
        vote_counts = defaultdict(int)

        for agent, result in zip(active_agents, results):
            if isinstance(result, Exception):
                vote = AgentVote(
                    agent_id=agent.agent_id,
                    llm_name=agent.llm_name,
                    weight=agent.weight,
                    raw_response=f"ERROR: {str(result)}",
                    extracted_answer="ERROR",
                    response_time=0.0,
                    is_correct=False
                )
            else:
                response, resp_time = result
                extracted = self._extract_answer(response)
                vote = AgentVote(
                    agent_id=agent.agent_id,
                    llm_name=agent.llm_name,
                    weight=agent.weight,
                    raw_response=response,
                    extracted_answer=extracted,
                    response_time=resp_time,
                    is_correct=(extracted == correct_answer)
                )

                if extracted not in ["INVALID", "ERROR"]:
                    vote_scores[extracted] += agent.weight
                    vote_counts[extracted] += 1

            record.agent_votes.append(vote)

        # è®¡ç®—æœ€ç»ˆç­”æ¡ˆå’Œç»Ÿè®¡
        record.vote_distribution = dict(vote_scores)
        record.raw_vote_counts = dict(vote_counts)

        if vote_scores:
            record.final_answer = max(vote_scores.items(), key=lambda x: x[1])[0]
        else:
            record.final_answer = "INVALID"

        record.is_correct = (record.final_answer == correct_answer)

        # ä¸€è‡´æ€§æŒ‡æ ‡
        total_valid_votes = sum(vote_counts.values())
        if total_valid_votes > 0:
            max_votes = max(vote_counts.values())
            record.is_unanimous = (len(vote_counts) == 1)
            record.agreement_ratio = max_votes / total_valid_votes

            # è®¡ç®—ç†µ
            probs = [c / total_valid_votes for c in vote_counts.values()]
            record.entropy = -sum(p * math.log2(p) if p > 0 else 0 for p in probs)

        record.total_time = time.time() - start_time

        return record

    def _extract_answer(self, response: str) -> str:
        """ä»å›å¤ä¸­æå–ç­”æ¡ˆå­—æ¯"""

        # ç­–ç•¥1: **Answer: X** æ ¼å¼
        match = re.search(r'\*\*Answer:\s*([A-D])\*\*', response, re.IGNORECASE)
        if match:
            return match.group(1).upper()

        # ç­–ç•¥2: Answer: X æ ¼å¼
        match = re.search(r'(?:Answer|ç­”æ¡ˆ):\s*([A-D])', response, re.IGNORECASE)
        if match:
            return match.group(1).upper()

        # ç­–ç•¥3: </think>åçš„å†…å®¹
        think_split = response.split('</think>')
        if len(think_split) > 1:
            after_think = think_split[-1]
            match = re.search(r'(?:^|\s|[.!?\n])\s*([A-D])(?:\s|[.!?,\n]|$)', after_think, re.MULTILINE | re.IGNORECASE)
            if match:
                return match.group(1).upper()
            for char in after_think:
                if char.upper() in ['A', 'B', 'C', 'D']:
                    return char.upper()

        # ç­–ç•¥4: æœ€åä¸€è¡Œçš„ç‹¬ç«‹å­—æ¯
        lines = response.strip().split('\n')
        for line in reversed(lines):
            line = line.strip()
            if len(line) == 1 and line.upper() in ['A', 'B', 'C', 'D']:
                return line.upper()

        # ç­–ç•¥5: "X is correct" æ¨¡å¼
        matches = re.findall(r'([A-D])\s*(?:is|ä¸º|æ˜¯)\s*(?:correct|right|æ­£ç¡®)', response, re.IGNORECASE)
        if matches:
            return matches[-1].upper()

        # ç­–ç•¥6: æ‰€æœ‰ç‹¬ç«‹å­—æ¯ä¸­çš„æœ€åä¸€ä¸ª
        all_letters = re.findall(r'(?:^|\s|[.!?\n])\s*([A-D])(?:\s|[.!?,\n]|$)', response, re.MULTILINE | re.IGNORECASE)
        if all_letters:
            return all_letters[-1].upper()

        # ç­–ç•¥7: æ–‡æœ¬ä¸­ç¬¬ä¸€ä¸ªå­—æ¯
        for char in response:
            if char.upper() in ['A', 'B', 'C', 'D']:
                return char.upper()

        return "INVALID"


# ============================================================================
# å®éªŒè¿è¡Œå™¨
# ============================================================================

class ExperimentRunner:
    """å®éªŒè¿è¡Œå™¨"""

    def __init__(
            self,
            llm_configs: List[Tuple[str, float]],
            dataset,
            is_homogeneous: bool = True,
            scan_mode: bool = False,
            wandb_run=None,
            temperature=0.7,
            enable_thinking=True
    ):
        self.llm_configs = llm_configs
        self.dataset = dataset
        self.is_homogeneous = is_homogeneous
        self.scan_mode = scan_mode
        self.wandb_run = wandb_run

        # åˆå§‹åŒ–æŠ•ç¥¨ç³»ç»Ÿ
        self.voting_system = MultiLLMVotingSystemV2(llm_configs, temperature=temperature,
                                                    enable_thinking=enable_thinking)

        # å®éªŒå…ƒæ•°æ®
        self.experiment_id = time.strftime("%Y%m%d_%H%M%S")
        self.metadata = ExperimentMetadata(
            experiment_id=self.experiment_id,
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
            config={},
            llm_configs=llm_configs,
            is_homogeneous=is_homogeneous,
            dataset_name="MMLU",
            dataset_split=dataset.split,
            total_questions=0
        )

    async def run(
            self,
            limit_questions: Optional[int] = None,
            batch_size: int = 4,
            debug_first_n: int = 0
    ) -> ExperimentMetadata:
        """
        è¿è¡Œå®éªŒ

        Args:
            limit_questions: é™åˆ¶é—®é¢˜æ•°é‡
            batch_size: æ‰¹å¤„ç†å¤§å°
            debug_first_n: å‰Nä¸ªé—®é¢˜å¯ç”¨è°ƒè¯•
        """
        total_questions = min(len(self.dataset), limit_questions) if limit_questions else len(self.dataset)
        self.metadata.total_questions = total_questions

        print(f"\n{'=' * 80}")
        print(f"RUNNING EXPERIMENT: {self.experiment_id}")
        print(f"{'=' * 80}")
        print(f"Total Agents: {len(self.voting_system.agents)}")
        print(f"Total Questions: {total_questions}")
        print(f"Scan Mode: {self.scan_mode}")
        print(f"{'=' * 80}\n")

        # é‡ç½®è®¡æ•°å™¨
        Cost.instance().reset()
        PromptTokens.instance().reset()
        CompletionTokens.instance().reset()

        start_time = time.time()

        # æ”¶é›†æ‰€æœ‰é—®é¢˜çš„è®°å½•
        question_records = []
        num_batches = math.ceil(total_questions / batch_size)

        for batch_idx in tqdm(range(num_batches), desc="Processing"):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total_questions)

            batch_tasks = []
            for idx in range(start_idx, end_idx):
                record = self.dataset[idx]
                input_dict = self.dataset.record_to_input(record)
                question = input_dict['task']
                correct_answer = self.dataset.record_to_target_answer(record)

                task = self.voting_system.vote_on_question(
                    question_id=idx,
                    question=question,
                    correct_answer=correct_answer
                )
                batch_tasks.append(task)

            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            for result in batch_results:
                if isinstance(result, Exception):
                    print(f"Error: {result}")
                else:
                    question_records.append(result)

                    # WandBå®æ—¶è®°å½•
                    if self.wandb_run:
                        self.wandb_run.log({
                            "question/is_correct": int(result.is_correct),
                            "question/agreement_ratio": result.agreement_ratio,
                            "question/entropy": result.entropy,
                            "question/is_unanimous": int(result.is_unanimous),
                            "question/time": result.total_time
                        })

            # æ‰“å°è¿›åº¦
            if (batch_idx + 1) % 5 == 0:
                correct = sum(1 for r in question_records if r.is_correct)
                print(f"\nProgress: {len(question_records)}/{total_questions}, "
                      f"Accuracy: {correct / len(question_records):.2%}")

        self.metadata.question_records = question_records
        self.metadata.total_time = time.time() - start_time
        self.metadata.total_cost = Cost.instance().value
        self.metadata.total_prompt_tokens = int(PromptTokens.instance().value)
        self.metadata.total_completion_tokens = int(CompletionTokens.instance().value)

        # æ‰«ææ¨¡å¼ï¼šè®¡ç®—ä¸åŒagentæ•°é‡çš„ç»“æœ
        if self.scan_mode:
            self._compute_scan_results()

        # æ‰“å°æ±‡æ€»
        self._print_summary()

        # WandBè®°å½•æ±‡æ€»
        if self.wandb_run:
            self._log_to_wandb()

        return self.metadata

    def _compute_scan_results(self):
        """è®¡ç®—æ‰«ææ¨¡å¼çš„ç»“æœ"""
        print("\n" + "=" * 80)
        print("COMPUTING SCAN RESULTS")
        print("=" * 80)

        num_agents = len(self.voting_system.agents)

        for n in range(1, num_agents + 1):
            # è·å–å‰nä¸ªagentçš„ID
            agent_ids = [agent.agent_id for agent in self.voting_system.agents[:n]]

            # é‡æ–°è®¡ç®—æ¯é“é¢˜çš„ç»“æœ
            correct_count = 0
            unanimous_count = 0
            total_agreement = 0.0
            total_time = 0.0

            for record in self.metadata.question_records:
                # ç­›é€‰å‰nä¸ªagentçš„æŠ•ç¥¨
                subset_votes = [v for v in record.agent_votes if v.agent_id in agent_ids]

                # é‡æ–°è®¡ç®—åŠ æƒå¾—åˆ†
                vote_scores = defaultdict(float)
                vote_counts = defaultdict(int)
                total_weight = sum(v.weight for v in subset_votes)

                for vote in subset_votes:
                    if vote.extracted_answer not in ["INVALID", "ERROR"]:
                        normalized_weight = vote.weight / total_weight if total_weight > 0 else 0
                        vote_scores[vote.extracted_answer] += normalized_weight
                        vote_counts[vote.extracted_answer] += 1

                # æœ€ç»ˆç­”æ¡ˆ
                if vote_scores:
                    final_answer = max(vote_scores.items(), key=lambda x: x[1])[0]
                    is_correct = (final_answer == record.correct_answer)
                else:
                    is_correct = False

                correct_count += int(is_correct)

                # ä¸€è‡´æ€§
                total_valid = sum(vote_counts.values())
                if total_valid > 0:
                    unanimous_count += int(len(vote_counts) == 1)
                    total_agreement += max(vote_counts.values()) / total_valid

                total_time += record.total_time

            total_questions = len(self.metadata.question_records)

            scan_result = ScanResult(
                num_agents=n,
                agent_ids=agent_ids,
                accuracy=correct_count / total_questions if total_questions > 0 else 0,
                correct_count=correct_count,
                total_count=total_questions,
                unanimous_ratio=unanimous_count / total_questions if total_questions > 0 else 0,
                avg_agreement_ratio=total_agreement / total_questions if total_questions > 0 else 0,
                avg_time=total_time / total_questions if total_questions > 0 else 0
            )

            self.metadata.scan_results.append(scan_result)

            print(f"  Agents={n}: Accuracy={scan_result.accuracy:.2%} "
                  f"({correct_count}/{total_questions}), "
                  f"Unanimous={scan_result.unanimous_ratio:.1%}")

    def _print_summary(self):
        """æ‰“å°å®éªŒæ±‡æ€»"""
        print("\n" + "=" * 80)
        print("EXPERIMENT SUMMARY")
        print("=" * 80)

        records = self.metadata.question_records
        correct = sum(1 for r in records if r.is_correct)
        unanimous = sum(1 for r in records if r.is_unanimous)

        print(f"\nğŸ“Š Overall Performance:")
        print(f"  Accuracy: {correct / len(records):.2%} ({correct}/{len(records)})")
        print(f"  Unanimous Votes: {unanimous / len(records):.1%}")
        print(f"  Avg Agreement: {sum(r.agreement_ratio for r in records) / len(records):.2%}")
        print(f"  Avg Entropy: {sum(r.entropy for r in records) / len(records):.3f}")

        print(f"\nğŸ¤– Per-Agent Performance:")
        agent_stats = defaultdict(lambda: {'correct': 0, 'total': 0})
        for record in records:
            for vote in record.agent_votes:
                agent_stats[vote.agent_id]['total'] += 1
                if vote.is_correct:
                    agent_stats[vote.agent_id]['correct'] += 1

        for agent_id in sorted(agent_stats.keys()):
            stats = agent_stats[agent_id]
            acc = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
            print(f"  {agent_id}: {acc:.2%} ({stats['correct']}/{stats['total']})")

        print(f"\nğŸ’° Cost:")
        print(f"  Total Cost: ${self.metadata.total_cost:.4f}")
        print(f"  Prompt Tokens: {self.metadata.total_prompt_tokens / 1000:.1f}k")
        print(f"  Completion Tokens: {self.metadata.total_completion_tokens / 1000:.1f}k")
        print(f"  Total Time: {self.metadata.total_time:.1f}s")

        if self.metadata.scan_results:
            print(f"\nğŸ“ˆ Scan Results (Accuracy by # of Agents):")
            for sr in self.metadata.scan_results:
                bar = "â–ˆ" * int(sr.accuracy * 20)
                print(f"  {sr.num_agents:2d} agents: {bar:<20} {sr.accuracy:.2%}")

        print("\n" + "=" * 80)

    def _log_to_wandb(self):
        """è®°å½•åˆ°WandB"""
        records = self.metadata.question_records
        correct = sum(1 for r in records if r.is_correct)

        self.wandb_run.log({
            "summary/accuracy": correct / len(records),
            "summary/unanimous_ratio": sum(1 for r in records if r.is_unanimous) / len(records),
            "summary/avg_agreement": sum(r.agreement_ratio for r in records) / len(records),
            "summary/avg_entropy": sum(r.entropy for r in records) / len(records),
            "summary/total_cost": self.metadata.total_cost,
            "summary/total_time": self.metadata.total_time
        })

        # æ¯ä¸ªagentçš„å‡†ç¡®ç‡
        agent_stats = defaultdict(lambda: {'correct': 0, 'total': 0})
        for record in records:
            for vote in record.agent_votes:
                agent_stats[vote.agent_id]['total'] += 1
                if vote.is_correct:
                    agent_stats[vote.agent_id]['correct'] += 1

        for agent_id, stats in agent_stats.items():
            acc = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
            self.wandb_run.log({f"agent/{agent_id}_accuracy": acc})

        # æ‰«æç»“æœ
        if self.metadata.scan_results:
            # åˆ›å»ºè¡¨æ ¼æ•°æ®
            scan_data = [[sr.num_agents, sr.accuracy, sr.unanimous_ratio]
                         for sr in self.metadata.scan_results]

            import wandb
            table = wandb.Table(
                data=scan_data,
                columns=["num_agents", "accuracy", "unanimous_ratio"]
            )
            self.wandb_run.log({"scan_results": table})

            # åˆ›å»ºæŠ˜çº¿å›¾
            for sr in self.metadata.scan_results:
                self.wandb_run.log({
                    "scan/accuracy": sr.accuracy,
                    "scan/num_agents": sr.num_agents
                })

    def save_results(self, output_dir: Path):
        """ä¿å­˜å®éªŒç»“æœ"""
        output_dir.mkdir(parents=True, exist_ok=True)

        # 1. ä¿å­˜JSONæ±‡æ€»
        json_path = output_dir / f"experiment_{self.experiment_id}.json"
        json_data = {
            "experiment_id": self.metadata.experiment_id,
            "timestamp": self.metadata.timestamp,
            "config": self.metadata.config,
            "is_homogeneous": self.metadata.is_homogeneous,
            "llm_configs": self.metadata.llm_configs,
            "total_questions": self.metadata.total_questions,
            "total_cost": self.metadata.total_cost,
            "total_time": self.metadata.total_time,
            "summary": {
                "accuracy": sum(1 for r in self.metadata.question_records if r.is_correct) / len(
                    self.metadata.question_records),
                "unanimous_ratio": sum(1 for r in self.metadata.question_records if r.is_unanimous) / len(
                    self.metadata.question_records),
            },
            "scan_results": [asdict(sr) for sr in self.metadata.scan_results] if self.metadata.scan_results else None
        }

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        print(f"âœ“ JSON summary saved to: {json_path}")

        # 2. ä¿å­˜å®Œæ•´å…ƒæ•°æ®ï¼ˆpickleï¼‰
        pickle_path = output_dir / f"metadata_{self.experiment_id}.pkl"
        with open(pickle_path, 'wb') as f:
            pickle.dump(self.metadata, f)
        print(f"âœ“ Full metadata saved to: {pickle_path}")

        return json_path, pickle_path


# ============================================================================
# é…ç½®å·¥å…·å‡½æ•°
# ============================================================================

def create_homogeneous_config(llm_name: str, num_agents: int, weights: Optional[List[float]] = None) -> List[
    Tuple[str, float]]:
    """åˆ›å»ºåŒæ„LLMé…ç½®"""
    if weights is None:
        weights = [1.0] * num_agents

    if len(weights) != num_agents:
        raise ValueError(f"Weights length ({len(weights)}) must match num_agents ({num_agents})")

    total = sum(weights)
    normalized_weights = [w / total for w in weights]

    return [(llm_name, w) for w in normalized_weights]


def create_heterogeneous_config(llm_names: List[str], weights: Optional[List[float]] = None) -> List[Tuple[str, float]]:
    """åˆ›å»ºå¼‚æ„LLMé…ç½®"""
    num_agents = len(llm_names)

    if weights is None:
        weights = [1.0] * num_agents

    if len(weights) != num_agents:
        raise ValueError(f"Weights length ({len(weights)}) must match number of LLMs ({num_agents})")

    total = sum(weights)
    normalized_weights = [w / total for w in weights]

    return list(zip(llm_names, normalized_weights))


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================

def parse_args():
    parser = argparse.ArgumentParser(
        description="Multi-LLM Weighted Voting System V2",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # åŒæ„LLM (5ä¸ªç›¸åŒæ¨¡å‹)
  python run_multi_llm_voting_v2.py --homogeneous --llm_name "Qwen/Qwen3-4B" --num_agents 5
  
  # åŒæ„LLM + æ‰«ææ¨¡å¼
  python run_multi_llm_voting_v2.py --homogeneous --llm_name "Qwen/Qwen3-4B" --num_agents 10 --scan_mode
  
  # å¼‚æ„LLM
  python run_multi_llm_voting_v2.py --heterogeneous --llm_names "Qwen/Qwen3-0.6B" "Qwen/Qwen3-1.7B" "Qwen/Qwen3-4B"
  
  # å¼‚æ„LLM + è‡ªå®šä¹‰æƒé‡
  python run_multi_llm_voting_v2.py --heterogeneous --llm_names "Qwen/Qwen3-0.6B" "Qwen/Qwen3-4B" --weights 0.3 0.7
        """
    )

    # é…ç½®æ¨¡å¼
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument('--homogeneous', action='store_true', help='åŒæ„LLMæ¨¡å¼')
    mode_group.add_argument('--heterogeneous', action='store_true', help='å¼‚æ„LLMæ¨¡å¼')

    # åŒæ„æ¨¡å¼å‚æ•°
    parser.add_argument('--llm_name', type=str, help='åŒæ„æ¨¡å¼ä¸‹çš„LLMåç§°')
    parser.add_argument('--num_agents', type=int, default=3, help='æ™ºèƒ½ä½“æ•°é‡')
    parser.add_argument("--temperature", default=0.7, type=float)
    parser.add_argument("--disable_thinking", action='store_true')

    # å¼‚æ„æ¨¡å¼å‚æ•°
    parser.add_argument('--llm_names', nargs='+', type=str, help='å¼‚æ„æ¨¡å¼ä¸‹çš„LLMåç§°åˆ—è¡¨')

    # æƒé‡
    parser.add_argument('--weights', nargs='+', type=float, default=None, help='è‡ªå®šä¹‰æƒé‡')

    # æ‰«ææ¨¡å¼
    parser.add_argument('--scan_mode', action='store_true', help='å¯ç”¨æ‰«ææ¨¡å¼')

    # å®éªŒå‚æ•°
    parser.add_argument('--limit', type=int, default=153, help='é™åˆ¶é—®é¢˜æ•°é‡')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--debug_first_n', type=int, default=0, help='å‰Nä¸ªé—®é¢˜å¯ç”¨è°ƒè¯•')

    # è¾“å‡º
    parser.add_argument('--output_dir', type=str, default=None, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--no_wandb', action='store_true', help='ç¦ç”¨WandB')
    parser.add_argument('--weave_project', type=str, default='vito_chan/Multi-LLM-Voting-V2', help='Weaveé¡¹ç›®å')

    return parser.parse_args()


async def main():
    args = parse_args()
    # args.llm_names = ["Qwen/Qwen3-1.7B", "Qwen/Qwen3-4B"] * 50
    # args.weights = [61.4, 76.5] * 50

    # éªŒè¯å‚æ•°
    if args.homogeneous and not args.llm_name:
        raise ValueError("åŒæ„æ¨¡å¼å¿…é¡»æŒ‡å®š --llm_name")
    if args.heterogeneous and not args.llm_names:
        raise ValueError("å¼‚æ„æ¨¡å¼å¿…é¡»æŒ‡å®š --llm_names")

    # åˆ›å»ºé…ç½®
    if args.homogeneous:
        llm_configs = create_homogeneous_config(args.llm_name, args.num_agents, args.weights)
        is_homogeneous = True
    else:
        llm_configs = create_heterogeneous_config(args.llm_names, args.weights)
        is_homogeneous = False

    print("\n" + "=" * 80)
    print("MULTI-LLM VOTING SYSTEM V2")
    print("=" * 80)
    print(f"Mode: {'Homogeneous' if is_homogeneous else 'Heterogeneous'}")
    print(f"LLM Configs:")
    for llm_name, weight in llm_configs:
        print(f"  - {llm_name}: weight={weight:.3f}")
    print(f"Scan Mode: {args.scan_mode}")
    print("=" * 80 + "\n")

    # åˆå§‹åŒ–è¿½è¸ª
    weave.init(project_name=args.weave_project)

    wandb_run = None
    if not args.no_wandb:
        import wandb
        wandb_run = wandb.init(
            project="Multi-LLM-Voting-V2",
            config=vars(args),
            name=f"{'homo' if is_homogeneous else 'hetero'}_{len(llm_configs)}agents_{time.strftime('%H%M%S')}"
        )

    # åŠ è½½æ•°æ®é›†
    download()
    dataset = MMLUDataset('val')

    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = ExperimentRunner(
        llm_configs=llm_configs,
        dataset=dataset,
        is_homogeneous=is_homogeneous,
        scan_mode=args.scan_mode,
        wandb_run=wandb_run,
        temperature=args.temperature,
        enable_thinking=not args.disable_thinking,
    )

    # è¿è¡Œå®éªŒ
    metadata = await runner.run(
        limit_questions=args.limit,
        batch_size=args.batch_size,
        debug_first_n=args.debug_first_n
    )

    # ä¿å­˜ç»“æœ
    output_dir = Path(args.output_dir) if args.output_dir else GDesigner_ROOT / "result" / "multi_llm_voting_v2"
    runner.save_results(output_dir)

    print("\nâœ… EXPERIMENT COMPLETED SUCCESSFULLY\n")

    if wandb_run:
        wandb_run.finish()


if __name__ == "__main__":
    asyncio.run(main())
