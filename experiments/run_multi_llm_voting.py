"""
å¤šLLMè¾©è®ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
æ”¯æŒåŒæ„/å¼‚æ„LLMé…ç½®ã€å¤šè½®è¾©è®ºã€æ‰«ææ¨¡å¼ã€è¯¦ç»†æ•°æ®è®°å½•

Features:
- åŒæ„LLM: æŒ‡å®šå•ä¸ªLLMåå­—å’Œæ•°é‡
- å¼‚æ„LLM: æŒ‡å®šLLMåå­—åˆ—è¡¨
- å¤šè½®è¾©è®º: æ¯è½®agentå¯ä»¥çœ‹åˆ°å…¶ä»–agentçš„å›ç­”å¹¶æ›´æ–°è‡ªå·±çš„ç­”æ¡ˆ
- å½“debate_rounds=1æ—¶ï¼Œè¡Œä¸ºä¸majority votingä¸€è‡´
- æ‰«ææ¨¡å¼: è‡ªåŠ¨æµ‹è¯•1åˆ°Nä¸ªLLMçš„æŠ•ç¥¨ç»“æœ
- è¯¦ç»†æ•°æ®è®°å½•: ä¿å­˜æ¯é“é¢˜çš„æŠ•ç¥¨åˆ†å¸ƒã€ç½®ä¿¡åº¦ç­‰å…ƒæ•°æ®
- WandBé›†æˆ: å®æ—¶è®°å½•å’Œå¯è§†åŒ–

Usage:
    # åŒæ„LLM (5ä¸ªç›¸åŒæ¨¡å‹), 3è½®è¾©è®º
    python run_multi_llm_debate.py --homogeneous --llm_name "Qwen/Qwen3-4B" --num_agents 5 --debate_rounds 3

    # å¼‚æ„LLM + è¾©è®º
    python run_multi_llm_debate.py --heterogeneous --llm_names "Qwen/Qwen3-0.6B" "Qwen/Qwen3-1.7B" "Qwen/Qwen3-4B" --debate_rounds 2

    # å•è½® (ç­‰åŒäºmajority voting)
    python run_multi_llm_debate.py --homogeneous --llm_name "Qwen/Qwen3-4B" --num_agents 5 --debate_rounds 1
"""

import sys
import os

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
sys.stdout.reconfigure(encoding='utf-8')

import asyncio
import argparse
import time
import json
import pickle
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field, asdict
from collections import Counter, defaultdict
from tqdm import tqdm
import math

import weave

# å¯¼å…¥é¡¹ç›®ä¾èµ–
from GDesigner.llm.llm_registry import LLMRegistry
from GDesigner.utils.globals import Cost, PromptTokens, CompletionTokens
from GDesigner.utils.const import GDesigner_ROOT
from dataset.mmlu_dataset import MMLUDataset
from dataset.MMLU.download import download


# ============================================================================
# æ•°æ®ç±»å®šä¹‰ - ç”¨äºç»“æ„åŒ–å­˜å‚¨å®éªŒæ•°æ®
# ============================================================================

@dataclass
class RoundVote:
    """å•è½®ä¸­å•ä¸ªæ™ºèƒ½ä½“çš„æŠ•ç¥¨è®°å½•"""
    round_num: int
    agent_id: str
    llm_name: str
    weight: float
    raw_response: str
    extracted_answer: str
    response_time: float
    is_correct: bool = False


@dataclass
class AgentDebateHistory:
    """å•ä¸ªæ™ºèƒ½ä½“åœ¨æ‰€æœ‰è½®æ¬¡çš„è¾©è®ºå†å²"""
    agent_id: str
    llm_name: str
    weight: float
    round_votes: List[RoundVote] = field(default_factory=list)

    @property
    def final_answer(self) -> str:
        if self.round_votes:
            return self.round_votes[-1].extracted_answer
        return "INVALID"

    @property
    def answer_changed(self) -> bool:
        """ç­”æ¡ˆæ˜¯å¦åœ¨è¾©è®ºè¿‡ç¨‹ä¸­æ”¹å˜"""
        if len(self.round_votes) < 2:
            return False
        first_answer = self.round_votes[0].extracted_answer
        return any(v.extracted_answer != first_answer for v in self.round_votes[1:])


@dataclass
class QuestionRecord:
    """å•é“é¢˜ç›®çš„å®Œæ•´è®°å½•"""
    question_id: int
    question_text: str
    correct_answer: str

    # è¾©è®ºå†å²
    agent_histories: List[AgentDebateHistory] = field(default_factory=list)
    num_debate_rounds: int = 1

    # æœ€ç»ˆæŠ•ç¥¨ç»Ÿè®¡
    final_answer: str = ""
    is_correct: bool = False
    vote_distribution: Dict[str, float] = field(default_factory=dict)
    raw_vote_counts: Dict[str, int] = field(default_factory=dict)

    # æ¯è½®çš„æŠ•ç¥¨åˆ†å¸ƒ
    round_vote_distributions: List[Dict[str, float]] = field(default_factory=list)
    round_accuracies: List[float] = field(default_factory=list)

    # ä¸€è‡´æ€§æŒ‡æ ‡
    is_unanimous: bool = False
    agreement_ratio: float = 0.0
    entropy: float = 0.0

    # è¾©è®ºåŠ¨æ€æŒ‡æ ‡
    answer_change_count: int = 0  # æœ‰å¤šå°‘agentæ”¹å˜äº†ç­”æ¡ˆ
    convergence_round: int = -1   # åœ¨å“ªä¸€è½®è¾¾æˆä¸€è‡´ï¼ˆ-1è¡¨ç¤ºæœªè¾¾æˆï¼‰

    # æ—¶é—´
    total_time: float = 0.0


@dataclass
class ScanResult:
    """æ‰«ææ¨¡å¼ä¸‹æŸä¸ªagentæ•°é‡çš„ç»“æœ"""
    num_agents: int
    agent_ids: List[str]
    accuracy: float
    correct_count: int
    total_count: int
    unanimous_ratio: float
    avg_agreement_ratio: float
    avg_time: float


@dataclass
class ExperimentMetadata:
    """å®éªŒå…ƒæ•°æ®"""
    experiment_id: str
    timestamp: str
    config: Dict[str, Any]

    # LLMé…ç½®
    llm_configs: List[Tuple[str, float]]
    is_homogeneous: bool

    # æ•°æ®é›†ä¿¡æ¯
    dataset_name: str
    dataset_split: str
    total_questions: int

    # ç»“æœæ±‡æ€»
    question_records: List[QuestionRecord] = field(default_factory=list)
    scan_results: List[ScanResult] = field(default_factory=list)

    # æ€§èƒ½æŒ‡æ ‡
    total_cost: float = 0.0
    total_prompt_tokens: int = 0
    total_completion_tokens: int = 0
    total_time: float = 0.0
    debate_rounds: int = 1


# ============================================================================
# æ ¸å¿ƒç±»
# ============================================================================

class DebateAgent:
    """å•ä¸ªè¾©è®ºæ™ºèƒ½ä½“"""

    def __init__(self, agent_id: str, llm_name: str, weight: float = 1.0,
                 temperature: float = 0.7, enable_thinking: bool = True):
        self.agent_id = agent_id
        self.llm_name = llm_name
        self.weight = weight
        self.llm = LLMRegistry.get(llm_name)
        self.temperature = temperature
        self.enable_thinking = enable_thinking

    def _get_initial_prompt(self) -> str:
        """è·å–ç¬¬ä¸€è½®çš„ç³»ç»Ÿæç¤º"""
        return """You are an expert at multiple-choice questions.
You will be given a question with 4 options (A, B, C, D).
Only one answer is correct.

IMPORTANT OUTPUT FORMAT:
1. You can use <think>...</think> tags to show your reasoning process
2. After your thinking, you MUST output your final answer in this EXACT format:
   **Answer: X**
   where X is one of A, B, C, or D

Example output format:
<think>
...
</think>

**Answer: B**"""

    def _get_debate_prompt(self) -> str:
        """è·å–è¾©è®ºè½®çš„ç³»ç»Ÿæç¤º"""
        return """You are an expert at multiple-choice questions participating in a debate.
You will be given a question with 4 options (A, B, C, D), along with other experts' answers and reasoning.
Only one answer is correct.

Consider other experts' perspectives carefully:
- If their reasoning is convincing, you may change your answer
- If you believe your original answer is correct, defend it with stronger reasoning
- Focus on the logical validity of arguments, not just majority opinion

IMPORTANT OUTPUT FORMAT:
1. You can use <think>...</think> tags to show your reasoning process
2. After your thinking, you MUST output your final answer in this EXACT format:
   **Answer: X**
   where X is one of A, B, C, or D

Example output format:
<think>
...
</think>

**Answer: B**"""

    async def initial_vote(self, question: str) -> Tuple[str, float]:
        """ç¬¬ä¸€è½®æŠ•ç¥¨ï¼ˆæ— å…¶ä»–agentä¿¡æ¯ï¼‰"""
        system_prompt = self._get_initial_prompt()
        user_prompt = f"{question}\n\nRemember: End your response with **Answer: X** where X is your chosen letter."

        messages = [
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': user_prompt}
        ]

        start_time = time.time()
        response = await self.llm.acomp(messages, temperature=self.temperature,
                                         enable_thinking=self.enable_thinking)
        elapsed_time = time.time() - start_time

        return response, elapsed_time

    async def debate_vote(self, question: str, other_responses: List[Dict[str, str]]) -> Tuple[str, float]:
        """
        è¾©è®ºè½®æŠ•ç¥¨ï¼ˆå¯ä»¥çœ‹åˆ°å…¶ä»–agentçš„å›ç­”ï¼‰

        Args:
            question: åŸå§‹é—®é¢˜
            other_responses: å…¶ä»–agentçš„å›ç­”åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« {'agent_id': str, 'answer': str, 'reasoning': str}
        """
        system_prompt = self._get_debate_prompt()

        # æ„å»ºå…¶ä»–ä¸“å®¶çš„å›ç­”ä¿¡æ¯
        others_info = "\n\n--- Other Experts' Responses ---\n"
        for resp in other_responses:
            others_info += f"\n**{resp['agent_id']}** chose **{resp['answer']}**"
            if resp.get('reasoning'):
                # æˆªå–æ¨ç†éƒ¨åˆ†ï¼ˆé¿å…å¤ªé•¿ï¼‰
                reasoning = resp['reasoning'][:500] + "..." if len(resp['reasoning']) > 500 else resp['reasoning']
                others_info += f":\n{reasoning}\n"
            else:
                others_info += "\n"
        others_info += "\n--- End of Other Experts' Responses ---\n"

        user_prompt = f"""{question}

{others_info}

Now, considering the above perspectives, provide your answer.
Remember: End your response with **Answer: X** where X is your chosen letter."""

        messages = [
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': user_prompt}
        ]

        start_time = time.time()
        response = await self.llm.acomp(messages, temperature=self.temperature,
                                         enable_thinking=self.enable_thinking)
        elapsed_time = time.time() - start_time

        return response, elapsed_time


class MultiLLMDebateSystem:
    """å¤šLLMè¾©è®ºç³»ç»Ÿ"""

    def __init__(self, llm_configs: List[Tuple[str, float]], debate_rounds: int = 1,
                 temperature: float = 0.7, enable_thinking: bool = True):
        """
        Args:
            llm_configs: List of (llm_name, weight) tuples
            debate_rounds: è¾©è®ºè½®æ•°ï¼Œ1è¡¨ç¤ºæ— è¾©è®ºï¼ˆç­‰åŒäºmajority votingï¼‰
        """
        self.agents: List[DebateAgent] = []
        self.debate_rounds = debate_rounds

        for idx, (llm_name, weight) in enumerate(llm_configs):
            agent_id = f"agent_{idx}_{llm_name.split('/')[-1]}"
            agent = DebateAgent(agent_id, llm_name, weight,
                               temperature=temperature, enable_thinking=enable_thinking)
            self.agents.append(agent)

        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(agent.weight for agent in self.agents)
        if total_weight > 0:
            for agent in self.agents:
                agent.weight /= total_weight

    def _extract_answer(self, response: str) -> str:
        """ä»å›å¤ä¸­æå–ç­”æ¡ˆå­—æ¯"""

        # ç­–ç•¥1: **Answer: X** æ ¼å¼
        match = re.search(r'\*\*Answer:\s*([A-D])\*\*', response, re.IGNORECASE)
        if match:
            return match.group(1).upper()

        # ç­–ç•¥2: Answer: X æ ¼å¼
        match = re.search(r'(?:Answer|ç­”æ¡ˆ):\s*([A-D])', response, re.IGNORECASE)
        if match:
            return match.group(1).upper()

        # ç­–ç•¥3: </think>åçš„å†…å®¹
        think_split = response.split('</think>')
        if len(think_split) > 1:
            after_think = think_split[-1]
            match = re.search(r'(?:^|\s|[.!?\n])\s*([A-D])(?:\s|[.!?,\n]|$)', after_think, re.MULTILINE | re.IGNORECASE)
            if match:
                return match.group(1).upper()
            for char in after_think:
                if char.upper() in ['A', 'B', 'C', 'D']:
                    return char.upper()

        # ç­–ç•¥4: æœ€åä¸€è¡Œçš„ç‹¬ç«‹å­—æ¯
        lines = response.strip().split('\n')
        for line in reversed(lines):
            line = line.strip()
            if len(line) == 1 and line.upper() in ['A', 'B', 'C', 'D']:
                return line.upper()

        # ç­–ç•¥5: "X is correct" æ¨¡å¼
        matches = re.findall(r'([A-D])\s*(?:is|ä¸º|æ˜¯)\s*(?:correct|right|æ­£ç¡®)', response, re.IGNORECASE)
        if matches:
            return matches[-1].upper()

        # ç­–ç•¥6: æ‰€æœ‰ç‹¬ç«‹å­—æ¯ä¸­çš„æœ€åä¸€ä¸ª
        all_letters = re.findall(r'(?:^|\s|[.!?\n])\s*([A-D])(?:\s|[.!?,\n]|$)', response, re.MULTILINE | re.IGNORECASE)
        if all_letters:
            return all_letters[-1].upper()

        # ç­–ç•¥7: æ–‡æœ¬ä¸­ç¬¬ä¸€ä¸ªå­—æ¯
        for char in response:
            if char.upper() in ['A', 'B', 'C', 'D']:
                return char.upper()

        return "INVALID"

    def _extract_reasoning(self, response: str) -> str:
        """ä»å›å¤ä¸­æå–æ¨ç†éƒ¨åˆ†"""
        # å°è¯•æå– <think>...</think> ä¸­çš„å†…å®¹
        match = re.search(r'<think>(.*?)</think>', response, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1).strip()

        # å¦åˆ™è¿”å› **Answer: ä¹‹å‰çš„å†…å®¹
        answer_match = re.search(r'\*\*Answer:', response, re.IGNORECASE)
        if answer_match:
            return response[:answer_match.start()].strip()

        return response[:300]  # è¿”å›å‰300å­—ç¬¦

    async def vote_on_question(
            self,
            question_id: int,
            question: str,
            correct_answer: str,
            active_agent_ids: Optional[List[str]] = None
    ) -> QuestionRecord:
        """
        å¯¹å•ä¸ªé—®é¢˜è¿›è¡Œè¾©è®ºæŠ•ç¥¨
        """
        record = QuestionRecord(
            question_id=question_id,
            question_text=question[:500],
            correct_answer=correct_answer,
            num_debate_rounds=self.debate_rounds
        )

        start_time = time.time()

        # ç¡®å®šå‚ä¸çš„agents
        active_agents = self.agents
        if active_agent_ids:
            active_agents = [a for a in self.agents if a.agent_id in active_agent_ids]

        # åˆå§‹åŒ–æ¯ä¸ªagentçš„è¾©è®ºå†å²
        agent_histories: Dict[str, AgentDebateHistory] = {}
        for agent in active_agents:
            agent_histories[agent.agent_id] = AgentDebateHistory(
                agent_id=agent.agent_id,
                llm_name=agent.llm_name,
                weight=agent.weight
            )

        # å­˜å‚¨æ¯è½®çš„å›ç­”ï¼ˆç”¨äºä¸‹ä¸€è½®è¾©è®ºï¼‰
        current_round_responses: Dict[str, Dict[str, str]] = {}

        # è¿›è¡Œå¤šè½®è¾©è®º
        for round_num in range(1, self.debate_rounds + 1):
            if round_num == 1:
                # ç¬¬ä¸€è½®ï¼šç‹¬ç«‹æŠ•ç¥¨
                tasks = [agent.initial_vote(question) for agent in active_agents]
            else:
                # åç»­è½®ï¼šè¾©è®ºæŠ•ç¥¨
                tasks = []
                for agent in active_agents:
                    # æ”¶é›†å…¶ä»–agentçš„å›ç­”
                    other_responses = [
                        current_round_responses[other_id]
                        for other_id in current_round_responses
                        if other_id != agent.agent_id
                    ]
                    tasks.append(agent.debate_vote(question, other_responses))

            results = await asyncio.gather(*tasks, return_exceptions=True)

            # å¤„ç†æœ¬è½®ç»“æœ
            round_vote_scores = defaultdict(float)
            round_vote_counts = defaultdict(int)
            current_round_responses = {}

            for agent, result in zip(active_agents, results):
                if isinstance(result, Exception):
                    vote = RoundVote(
                        round_num=round_num,
                        agent_id=agent.agent_id,
                        llm_name=agent.llm_name,
                        weight=agent.weight,
                        raw_response=f"ERROR: {str(result)}",
                        extracted_answer="ERROR",
                        response_time=0.0,
                        is_correct=False
                    )
                    current_round_responses[agent.agent_id] = {
                        'agent_id': agent.agent_id,
                        'answer': 'ERROR',
                        'reasoning': ''
                    }
                else:
                    response, resp_time = result
                    extracted = self._extract_answer(response)
                    reasoning = self._extract_reasoning(response)

                    vote = RoundVote(
                        round_num=round_num,
                        agent_id=agent.agent_id,
                        llm_name=agent.llm_name,
                        weight=agent.weight,
                        raw_response=response,
                        extracted_answer=extracted,
                        response_time=resp_time,
                        is_correct=(extracted == correct_answer)
                    )

                    current_round_responses[agent.agent_id] = {
                        'agent_id': agent.agent_id,
                        'answer': extracted,
                        'reasoning': reasoning
                    }

                    if extracted not in ["INVALID", "ERROR"]:
                        round_vote_scores[extracted] += agent.weight
                        round_vote_counts[extracted] += 1

                agent_histories[agent.agent_id].round_votes.append(vote)

            # è®°å½•æœ¬è½®çš„æŠ•ç¥¨åˆ†å¸ƒ
            record.round_vote_distributions.append(dict(round_vote_scores))

            # è®¡ç®—æœ¬è½®å‡†ç¡®ç‡
            round_correct = sum(1 for agent in active_agents
                               if agent_histories[agent.agent_id].round_votes[-1].is_correct)
            record.round_accuracies.append(round_correct / len(active_agents))

            # æ£€æŸ¥æ˜¯å¦è¾¾æˆä¸€è‡´
            if len(round_vote_counts) == 1 and record.convergence_round == -1:
                record.convergence_round = round_num

        # ä¿å­˜agentå†å²
        record.agent_histories = list(agent_histories.values())

        # è®¡ç®—æœ€ç»ˆç»“æœï¼ˆåŸºäºæœ€åä¸€è½®ï¼‰
        final_vote_scores = defaultdict(float)
        final_vote_counts = defaultdict(int)

        for history in record.agent_histories:
            if history.round_votes:
                final_vote = history.round_votes[-1]
                if final_vote.extracted_answer not in ["INVALID", "ERROR"]:
                    final_vote_scores[final_vote.extracted_answer] += history.weight
                    final_vote_counts[final_vote.extracted_answer] += 1

        record.vote_distribution = dict(final_vote_scores)
        record.raw_vote_counts = dict(final_vote_counts)

        if final_vote_scores:
            record.final_answer = max(final_vote_scores.items(), key=lambda x: x[1])[0]
        else:
            record.final_answer = "INVALID"

        record.is_correct = (record.final_answer == correct_answer)

        # ä¸€è‡´æ€§æŒ‡æ ‡
        total_valid_votes = sum(final_vote_counts.values())
        if total_valid_votes > 0:
            max_votes = max(final_vote_counts.values())
            record.is_unanimous = (len(final_vote_counts) == 1)
            record.agreement_ratio = max_votes / total_valid_votes

            # è®¡ç®—ç†µ
            probs = [c / total_valid_votes for c in final_vote_counts.values()]
            record.entropy = -sum(p * math.log2(p) if p > 0 else 0 for p in probs)

        # è¾©è®ºåŠ¨æ€æŒ‡æ ‡
        record.answer_change_count = sum(1 for h in record.agent_histories if h.answer_changed)

        record.total_time = time.time() - start_time

        return record


# ============================================================================
# å®éªŒè¿è¡Œå™¨
# ============================================================================

class ExperimentRunner:
    """å®éªŒè¿è¡Œå™¨"""

    def __init__(
            self,
            llm_configs: List[Tuple[str, float]],
            dataset,
            is_homogeneous: bool = True,
            debate_rounds: int = 1,
            scan_mode: bool = False,
            wandb_run=None,
            temperature: float = 0.7,
            enable_thinking: bool = True
    ):
        self.llm_configs = llm_configs
        self.dataset = dataset
        self.is_homogeneous = is_homogeneous
        self.debate_rounds = debate_rounds
        self.scan_mode = scan_mode
        self.wandb_run = wandb_run

        # åˆå§‹åŒ–è¾©è®ºç³»ç»Ÿ
        self.debate_system = MultiLLMDebateSystem(
            llm_configs,
            debate_rounds=debate_rounds,
            temperature=temperature,
            enable_thinking=enable_thinking
        )

        # å®éªŒå…ƒæ•°æ®
        self.experiment_id = time.strftime("%Y%m%d_%H%M%S")
        self.metadata = ExperimentMetadata(
            experiment_id=self.experiment_id,
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
            config={},
            llm_configs=llm_configs,
            is_homogeneous=is_homogeneous,
            debate_rounds=debate_rounds,
            dataset_name="MMLU",
            dataset_split=dataset.split,
            total_questions=0
        )

    async def run(
            self,
            limit_questions: Optional[int] = None,
            batch_size: int = 4,
            debug_first_n: int = 0
    ) -> ExperimentMetadata:
        """è¿è¡Œå®éªŒ"""
        total_questions = min(len(self.dataset), limit_questions) if limit_questions else len(self.dataset)
        self.metadata.total_questions = total_questions

        print(f"\n{'=' * 80}")
        print(f"RUNNING DEBATE EXPERIMENT: {self.experiment_id}")
        print(f"{'=' * 80}")
        print(f"Total Agents: {len(self.debate_system.agents)}")
        print(f"Debate Rounds: {self.debate_rounds}")
        print(f"Total Questions: {total_questions}")
        print(f"Scan Mode: {self.scan_mode}")
        print(f"{'=' * 80}\n")

        # é‡ç½®è®¡æ•°å™¨
        Cost.instance().reset()
        PromptTokens.instance().reset()
        CompletionTokens.instance().reset()

        start_time = time.time()

        # æ”¶é›†æ‰€æœ‰é—®é¢˜çš„è®°å½•
        question_records = []
        num_batches = math.ceil(total_questions / batch_size)

        for batch_idx in tqdm(range(num_batches), desc="Processing"):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total_questions)

            batch_tasks = []
            for idx in range(start_idx, end_idx):
                record = self.dataset[idx]
                input_dict = self.dataset.record_to_input(record)
                question = input_dict['task']
                correct_answer = self.dataset.record_to_target_answer(record)

                task = self.debate_system.vote_on_question(
                    question_id=idx,
                    question=question,
                    correct_answer=correct_answer
                )
                batch_tasks.append(task)

            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            for result in batch_results:
                if isinstance(result, Exception):
                    print(f"Error: {result}")
                else:
                    question_records.append(result)

                    # WandBå®æ—¶è®°å½•
                    if self.wandb_run:
                        log_data = {
                            "question/is_correct": int(result.is_correct),
                            "question/agreement_ratio": result.agreement_ratio,
                            "question/entropy": result.entropy,
                            "question/is_unanimous": int(result.is_unanimous),
                            "question/time": result.total_time,
                            "question/answer_changes": result.answer_change_count,
                        }
                        if result.convergence_round > 0:
                            log_data["question/convergence_round"] = result.convergence_round
                        self.wandb_run.log(log_data)

            # æ‰“å°è¿›åº¦
            if (batch_idx + 1) % 5 == 0:
                correct = sum(1 for r in question_records if r.is_correct)
                print(f"\nProgress: {len(question_records)}/{total_questions}, "
                      f"Accuracy: {correct / len(question_records):.2%}")

        self.metadata.question_records = question_records
        self.metadata.total_time = time.time() - start_time
        self.metadata.total_cost = Cost.instance().value
        self.metadata.total_prompt_tokens = int(PromptTokens.instance().value)
        self.metadata.total_completion_tokens = int(CompletionTokens.instance().value)

        # æ‰«ææ¨¡å¼
        if self.scan_mode:
            self._compute_scan_results()

        # æ‰“å°æ±‡æ€»
        self._print_summary()

        # WandBè®°å½•æ±‡æ€»
        if self.wandb_run:
            self._log_to_wandb()

        return self.metadata

    def _compute_scan_results(self):
        """è®¡ç®—æ‰«ææ¨¡å¼çš„ç»“æœ"""
        print("\n" + "=" * 80)
        print("COMPUTING SCAN RESULTS")
        print("=" * 80)

        num_agents = len(self.debate_system.agents)

        for n in range(1, num_agents + 1):
            agent_ids = [agent.agent_id for agent in self.debate_system.agents[:n]]

            correct_count = 0
            unanimous_count = 0
            total_agreement = 0.0
            total_time = 0.0

            for record in self.metadata.question_records:
                # ç­›é€‰å‰nä¸ªagentçš„å†å²
                subset_histories = [h for h in record.agent_histories if h.agent_id in agent_ids]

                # é‡æ–°è®¡ç®—åŠ æƒå¾—åˆ†ï¼ˆåŸºäºæœ€åä¸€è½®ï¼‰
                vote_scores = defaultdict(float)
                vote_counts = defaultdict(int)
                total_weight = sum(h.weight for h in subset_histories)

                for history in subset_histories:
                    if history.round_votes:
                        final_answer = history.round_votes[-1].extracted_answer
                        if final_answer not in ["INVALID", "ERROR"]:
                            normalized_weight = history.weight / total_weight if total_weight > 0 else 0
                            vote_scores[final_answer] += normalized_weight
                            vote_counts[final_answer] += 1

                if vote_scores:
                    final_answer = max(vote_scores.items(), key=lambda x: x[1])[0]
                    is_correct = (final_answer == record.correct_answer)
                else:
                    is_correct = False

                correct_count += int(is_correct)

                total_valid = sum(vote_counts.values())
                if total_valid > 0:
                    unanimous_count += int(len(vote_counts) == 1)
                    total_agreement += max(vote_counts.values()) / total_valid

                total_time += record.total_time

            total_questions = len(self.metadata.question_records)

            scan_result = ScanResult(
                num_agents=n,
                agent_ids=agent_ids,
                accuracy=correct_count / total_questions if total_questions > 0 else 0,
                correct_count=correct_count,
                total_count=total_questions,
                unanimous_ratio=unanimous_count / total_questions if total_questions > 0 else 0,
                avg_agreement_ratio=total_agreement / total_questions if total_questions > 0 else 0,
                avg_time=total_time / total_questions if total_questions > 0 else 0
            )

            self.metadata.scan_results.append(scan_result)

            print(f"  Agents={n}: Accuracy={scan_result.accuracy:.2%} "
                  f"({correct_count}/{total_questions}), "
                  f"Unanimous={scan_result.unanimous_ratio:.1%}")

    def _print_summary(self):
        """æ‰“å°å®éªŒæ±‡æ€»"""
        print("\n" + "=" * 80)
        print("EXPERIMENT SUMMARY")
        print("=" * 80)

        records = self.metadata.question_records
        correct = sum(1 for r in records if r.is_correct)
        unanimous = sum(1 for r in records if r.is_unanimous)

        print(f"\nğŸ“Š Overall Performance:")
        print(f"  Accuracy: {correct / len(records):.2%} ({correct}/{len(records)})")
        print(f"  Unanimous Votes: {unanimous / len(records):.1%}")
        print(f"  Avg Agreement: {sum(r.agreement_ratio for r in records) / len(records):.2%}")
        print(f"  Avg Entropy: {sum(r.entropy for r in records) / len(records):.3f}")

        # è¾©è®ºåŠ¨æ€
        if self.debate_rounds > 1:
            print(f"\nğŸ”„ Debate Dynamics:")
            avg_changes = sum(r.answer_change_count for r in records) / len(records)
            print(f"  Avg Answer Changes per Question: {avg_changes:.2f}")

            converged = [r for r in records if r.convergence_round > 0]
            if converged:
                avg_conv_round = sum(r.convergence_round for r in converged) / len(converged)
                print(f"  Questions Reaching Consensus: {len(converged)} ({len(converged)/len(records):.1%})")
                print(f"  Avg Convergence Round: {avg_conv_round:.2f}")

            # æ¯è½®å‡†ç¡®ç‡å˜åŒ–
            print(f"\nğŸ“ˆ Accuracy by Round:")
            for round_num in range(self.debate_rounds):
                round_accs = [r.round_accuracies[round_num] for r in records if len(r.round_accuracies) > round_num]
                if round_accs:
                    avg_acc = sum(round_accs) / len(round_accs)
                    print(f"  Round {round_num + 1}: {avg_acc:.2%}")

        print(f"\nğŸ¤– Per-Agent Performance (Final Round):")
        agent_stats = defaultdict(lambda: {'correct': 0, 'total': 0, 'changed': 0})
        for record in records:
            for history in record.agent_histories:
                agent_stats[history.agent_id]['total'] += 1
                if history.round_votes and history.round_votes[-1].is_correct:
                    agent_stats[history.agent_id]['correct'] += 1
                if history.answer_changed:
                    agent_stats[history.agent_id]['changed'] += 1

        for agent_id in sorted(agent_stats.keys()):
            stats = agent_stats[agent_id]
            acc = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
            change_rate = stats['changed'] / stats['total'] if stats['total'] > 0 else 0
            print(f"  {agent_id}: {acc:.2%} ({stats['correct']}/{stats['total']}), "
                  f"Changed: {change_rate:.1%}")

        print(f"\nğŸ’° Cost:")
        print(f"  Total Cost: ${self.metadata.total_cost:.4f}")
        print(f"  Prompt Tokens: {self.metadata.total_prompt_tokens / 1000:.1f}k")
        print(f"  Completion Tokens: {self.metadata.total_completion_tokens / 1000:.1f}k")
        print(f"  Total Time: {self.metadata.total_time:.1f}s")

        if self.metadata.scan_results:
            print(f"\nğŸ“ˆ Scan Results (Accuracy by # of Agents):")
            for sr in self.metadata.scan_results:
                bar = "â–ˆ" * int(sr.accuracy * 20)
                print(f"  {sr.num_agents:2d} agents: {bar:<20} {sr.accuracy:.2%}")

        print("\n" + "=" * 80)

    def _log_to_wandb(self):
        """è®°å½•åˆ°WandB"""
        records = self.metadata.question_records
        correct = sum(1 for r in records if r.is_correct)

        log_data = {
            "summary/accuracy": correct / len(records),
            "summary/unanimous_ratio": sum(1 for r in records if r.is_unanimous) / len(records),
            "summary/avg_agreement": sum(r.agreement_ratio for r in records) / len(records),
            "summary/avg_entropy": sum(r.entropy for r in records) / len(records),
            "summary/total_cost": self.metadata.total_cost,
            "summary/total_time": self.metadata.total_time,
            "summary/debate_rounds": self.debate_rounds,
        }

        if self.debate_rounds > 1:
            log_data["summary/avg_answer_changes"] = sum(r.answer_change_count for r in records) / len(records)

        self.wandb_run.log(log_data)

        # æ¯ä¸ªagentçš„å‡†ç¡®ç‡
        agent_stats = defaultdict(lambda: {'correct': 0, 'total': 0})
        for record in records:
            for history in record.agent_histories:
                agent_stats[history.agent_id]['total'] += 1
                if history.round_votes and history.round_votes[-1].is_correct:
                    agent_stats[history.agent_id]['correct'] += 1

        for agent_id, stats in agent_stats.items():
            acc = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
            self.wandb_run.log({f"agent/{agent_id}_accuracy": acc})

        # æ‰«æç»“æœ
        if self.metadata.scan_results:
            import wandb
            scan_data = [[sr.num_agents, sr.accuracy, sr.unanimous_ratio]
                         for sr in self.metadata.scan_results]
            table = wandb.Table(
                data=scan_data,
                columns=["num_agents", "accuracy", "unanimous_ratio"]
            )
            self.wandb_run.log({"scan_results": table})

            for sr in self.metadata.scan_results:
                self.wandb_run.log({
                    "scan/accuracy": sr.accuracy,
                    "scan/num_agents": sr.num_agents
                })

    def save_results(self, output_dir: Path):
        """ä¿å­˜å®éªŒç»“æœ"""
        output_dir.mkdir(parents=True, exist_ok=True)

        # 1. ä¿å­˜JSONæ±‡æ€»
        json_path = output_dir / f"debate_experiment_{self.experiment_id}.json"
        json_data = {
            "experiment_id": self.metadata.experiment_id,
            "timestamp": self.metadata.timestamp,
            "config": self.metadata.config,
            "is_homogeneous": self.metadata.is_homogeneous,
            "debate_rounds": self.metadata.debate_rounds,
            "llm_configs": self.metadata.llm_configs,
            "total_questions": self.metadata.total_questions,
            "total_cost": self.metadata.total_cost,
            "total_time": self.metadata.total_time,
            "summary": {
                "accuracy": sum(1 for r in self.metadata.question_records if r.is_correct) / len(
                    self.metadata.question_records),
                "unanimous_ratio": sum(1 for r in self.metadata.question_records if r.is_unanimous) / len(
                    self.metadata.question_records),
            },
            "scan_results": [asdict(sr) for sr in self.metadata.scan_results] if self.metadata.scan_results else None
        }

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        print(f"âœ“ JSON summary saved to: {json_path}")

        # 2. ä¿å­˜å®Œæ•´å…ƒæ•°æ®ï¼ˆpickleï¼‰
        pickle_path = output_dir / f"debate_metadata_{self.experiment_id}.pkl"
        with open(pickle_path, 'wb') as f:
            pickle.dump(self.metadata, f)
        print(f"âœ“ Full metadata saved to: {pickle_path}")

        return json_path, pickle_path


# ============================================================================
# é…ç½®å·¥å…·å‡½æ•°
# ============================================================================

def create_homogeneous_config(llm_name: str, num_agents: int, weights: Optional[List[float]] = None) -> List[Tuple[str, float]]:
    """åˆ›å»ºåŒæ„LLMé…ç½®"""
    if weights is None:
        weights = [1.0] * num_agents

    if len(weights) != num_agents:
        raise ValueError(f"Weights length ({len(weights)}) must match num_agents ({num_agents})")

    total = sum(weights)
    normalized_weights = [w / total for w in weights]

    return [(llm_name, w) for w in normalized_weights]


def create_heterogeneous_config(llm_names: List[str], weights: Optional[List[float]] = None) -> List[Tuple[str, float]]:
    """åˆ›å»ºå¼‚æ„LLMé…ç½®"""
    num_agents = len(llm_names)

    if weights is None:
        weights = [1.0] * num_agents

    if len(weights) != num_agents:
        raise ValueError(f"Weights length ({len(weights)}) must match number of LLMs ({num_agents})")

    total = sum(weights)
    normalized_weights = [w / total for w in weights]

    return list(zip(llm_names, normalized_weights))


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================

def parse_args():
    parser = argparse.ArgumentParser(
        description="Multi-LLM Debate System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # åŒæ„LLM (5ä¸ªç›¸åŒæ¨¡å‹), 3è½®è¾©è®º
  python run_multi_llm_debate.py --homogeneous --llm_name "Qwen/Qwen3-4B" --num_agents 5 --debate_rounds 3
  
  # å•è½®ï¼ˆç­‰åŒäºmajority votingï¼‰
  python run_multi_llm_debate.py --homogeneous --llm_name "Qwen/Qwen3-4B" --num_agents 5 --debate_rounds 1
  
  # å¼‚æ„LLM + è¾©è®º
  python run_multi_llm_debate.py --heterogeneous --llm_names "Qwen/Qwen3-0.6B" "Qwen/Qwen3-1.7B" --debate_rounds 2
        """
    )

    # é…ç½®æ¨¡å¼
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument('--homogeneous', action='store_true', help='åŒæ„LLMæ¨¡å¼')
    mode_group.add_argument('--heterogeneous', action='store_true', help='å¼‚æ„LLMæ¨¡å¼')

    # åŒæ„æ¨¡å¼å‚æ•°
    parser.add_argument('--llm_name', type=str, help='åŒæ„æ¨¡å¼ä¸‹çš„LLMåç§°')
    parser.add_argument('--num_agents', type=int, default=3, help='æ™ºèƒ½ä½“æ•°é‡')
    parser.add_argument("--temperature", default=0.7, type=float)
    parser.add_argument("--disable_thinking", action='store_true')

    # å¼‚æ„æ¨¡å¼å‚æ•°
    parser.add_argument('--llm_names', nargs='+', type=str, help='å¼‚æ„æ¨¡å¼ä¸‹çš„LLMåç§°åˆ—è¡¨')

    # æƒé‡
    parser.add_argument('--weights', nargs='+', type=float, default=None, help='è‡ªå®šä¹‰æƒé‡')

    # è¾©è®ºå‚æ•°
    parser.add_argument('--debate_rounds', type=int, default=1, help='è¾©è®ºè½®æ•°ï¼ˆ1=æ— è¾©è®ºï¼Œç­‰åŒäºmajority votingï¼‰')

    # æ‰«ææ¨¡å¼
    parser.add_argument('--scan_mode', action='store_true', help='å¯ç”¨æ‰«ææ¨¡å¼')

    # å®éªŒå‚æ•°
    parser.add_argument('--limit', type=int, default=153, help='é™åˆ¶é—®é¢˜æ•°é‡')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--debug_first_n', type=int, default=0, help='å‰Nä¸ªé—®é¢˜å¯ç”¨è°ƒè¯•')

    # è¾“å‡º
    parser.add_argument('--output_dir', type=str, default=None, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--no_wandb', action='store_true', help='ç¦ç”¨WandB')
    parser.add_argument('--weave_project', type=str, default='vito_chan/Multi-LLM-Debate', help='Weaveé¡¹ç›®å')

    return parser.parse_args()


async def main():
    args = parse_args()

    # éªŒè¯å‚æ•°
    if args.homogeneous and not args.llm_name:
        raise ValueError("åŒæ„æ¨¡å¼å¿…é¡»æŒ‡å®š --llm_name")
    if args.heterogeneous and not args.llm_names:
        raise ValueError("å¼‚æ„æ¨¡å¼å¿…é¡»æŒ‡å®š --llm_names")

    # åˆ›å»ºé…ç½®
    if args.homogeneous:
        llm_configs = create_homogeneous_config(args.llm_name, args.num_agents, args.weights)
        is_homogeneous = True
    else:
        llm_configs = create_heterogeneous_config(args.llm_names, args.weights)
        is_homogeneous = False

    print("\n" + "=" * 80)
    print("MULTI-LLM DEBATE SYSTEM")
    print("=" * 80)
    print(f"Mode: {'Homogeneous' if is_homogeneous else 'Heterogeneous'}")
    print(f"Debate Rounds: {args.debate_rounds} {'(equivalent to majority voting)' if args.debate_rounds == 1 else ''}")
    print(f"LLM Configs:")
    for llm_name, weight in llm_configs:
        print(f"  - {llm_name}: weight={weight:.3f}")
    print(f"Scan Mode: {args.scan_mode}")
    print("=" * 80 + "\n")

    # åˆå§‹åŒ–è¿½è¸ª
    weave.init(project_name=args.weave_project)

    wandb_run = None
    if not args.no_wandb:
        import wandb
        wandb_run = wandb.init(
            project="Multi-LLM-Debate",
            config=vars(args),
            name=f"{'homo' if is_homogeneous else 'hetero'}_{len(llm_configs)}agents_r{args.debate_rounds}_{time.strftime('%H%M%S')}"
        )

    # åŠ è½½æ•°æ®é›†
    download()
    dataset = MMLUDataset('val')

    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = ExperimentRunner(
        llm_configs=llm_configs,
        dataset=dataset,
        is_homogeneous=is_homogeneous,
        debate_rounds=args.debate_rounds,
        scan_mode=args.scan_mode,
        wandb_run=wandb_run,
        temperature=args.temperature,
        enable_thinking=not args.disable_thinking,
    )

    # è¿è¡Œå®éªŒ
    metadata = await runner.run(
        limit_questions=args.limit,
        batch_size=args.batch_size,
        debug_first_n=args.debug_first_n
    )

    # ä¿å­˜ç»“æœ
    output_dir = Path(args.output_dir) if args.output_dir else GDesigner_ROOT / "result" / "multi_llm_debate"
    runner.save_results(output_dir)

    print("\nâœ… EXPERIMENT COMPLETED SUCCESSFULLY\n")

    if wandb_run:
        wandb_run.finish()


if __name__ == "__main__":
    asyncio.run(main())