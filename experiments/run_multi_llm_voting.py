"""
å¤šLLMåŠ æƒæŠ•ç¥¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
æ”¯æŒä¸åŒè§„æ¨¡çš„Qwenæ¨¡å‹è¿›è¡Œåä½œæ¨ç†å’ŒæŠ•ç¥¨å†³ç­–

Usage:
    python experiments/run_multi_llm_voting.py --num_agents 3 --limit 100
    python experiments/run_multi_llm_voting.py --num_agents 6 --weights 0.1 0.15 0.2 0.1 0.15 0.3
"""

import sys
import os

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
sys.stdout.reconfigure(encoding='utf-8')

import asyncio
import argparse
import time
import json
from pathlib import Path
from typing import Dict, List, Tuple
from tqdm import tqdm
from collections import Counter
import math

import weave

from GDesigner.llm.llm_registry import LLMRegistry
from GDesigner.utils.globals import Cost, PromptTokens, CompletionTokens
from GDesigner.utils.const import GDesigner_ROOT
from dataset.mmlu_dataset import MMLUDataset
from dataset.MMLU.download import download


class WeightedVotingAgent:
    """å•ä¸ªæŠ•ç¥¨æ™ºèƒ½ä½“ï¼ŒåŸºäºç‰¹å®šLLMæ¨¡å‹"""

    def __init__(self, agent_id: str, llm_name: str, weight: float, domain: str = "mmlu", debug: bool = False):
        self.agent_id = agent_id
        self.llm_name = llm_name
        self.weight = weight
        self.domain = domain
        self.debug = debug  # è°ƒè¯•æ¨¡å¼
        self.llm = LLMRegistry.get(llm_name)

        # å¯¼å…¥prompt
        from GDesigner.prompt.prompt_set_registry import PromptSetRegistry
        self.prompt_set = PromptSetRegistry.get(domain)

        print(f"  âœ“ Agent [{agent_id}] initialized: {llm_name} (weight={weight:.2f})")

    async def vote(self, question: str) -> str:
        """å¯¹é—®é¢˜è¿›è¡ŒæŠ•ç¥¨ï¼ˆç”Ÿæˆç­”æ¡ˆï¼‰"""

        # ä¼˜åŒ–çš„promptï¼šæ˜ç¡®è¦æ±‚åœ¨æ€è€ƒåè¾“å‡ºæ ¼å¼åŒ–ç­”æ¡ˆ
        system_prompt = """You are an expert at multiple-choice questions.
You will be given a question with 4 options (A, B, C, D).
Only one answer is correct.

IMPORTANT OUTPUT FORMAT:
1. You can use <think>...</think> tags to show your reasoning process
2. After your thinking, you MUST output your final answer in this EXACT format:
   **Answer: X**
   where X is one of A, B, C, or D

Example output format:
<think>
Let me analyze this question...
Option A seems wrong because...
Option B is correct because...
</think>

**Answer: B**"""

        user_prompt = f"{question}\n\nRemember: End your response with **Answer: X** where X is your chosen letter."

        messages = [
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': user_prompt}
        ]

        # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
        response = await self.llm.agen(messages, temperature=0.7)

        return response


class MultiLLMVotingSystem:
    """å¤šLLMåŠ æƒæŠ•ç¥¨ç³»ç»Ÿ"""

    def __init__(self,
                 llm_configs: List[Tuple[str, float]],
                 domain: str = "mmlu"):
        """
        Args:
            llm_configs: List of (llm_name, weight) tuples
            domain: æ•°æ®é›†é¢†åŸŸ
        """
        self.domain = domain
        self.agents = []

        print("\n" + "=" * 80)
        print("INITIALIZING MULTI-LLM VOTING SYSTEM")
        print("=" * 80)

        # åˆå§‹åŒ–æ‰€æœ‰æ™ºèƒ½ä½“
        for idx, (llm_name, weight) in enumerate(llm_configs):
            agent_id = f"agent_{idx}_{llm_name.split('/')[-1]}"
            agent = WeightedVotingAgent(
                agent_id=agent_id,
                llm_name=llm_name,
                weight=weight,
                domain=domain
            )
            self.agents.append(agent)

        # éªŒè¯æƒé‡æ€»å’Œ
        total_weight = sum(agent.weight for agent in self.agents)
        print(f"\n  Total weight: {total_weight:.2f}")

        if abs(total_weight - 1.0) > 0.01:
            print(f"  âš ï¸  Warning: Weights don't sum to 1.0, normalizing...")
            for agent in self.agents:
                agent.weight /= total_weight

        print("=" * 80 + "\n")

    async def vote_on_question(self, question: str, debug: bool = False) -> Tuple[str, Dict]:
        """
        å¯¹å•ä¸ªé—®é¢˜è¿›è¡ŒæŠ•ç¥¨

        Args:
            question: é—®é¢˜æ–‡æœ¬
            debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯

        Returns:
            (final_answer, voting_details)
        """
        # å¹¶å‘æ”¶é›†æ‰€æœ‰æ™ºèƒ½ä½“çš„æŠ•ç¥¨
        tasks = [agent.vote(question) for agent in self.agents]
        responses = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸å’Œæå–ç­”æ¡ˆ
        votes = []
        debug_info = []

        for agent, response in zip(self.agents, responses):
            if isinstance(response, Exception):
                print(f"  âš ï¸  {agent.agent_id} failed: {response}")
                votes.append(("ERROR", agent.weight))
                debug_info.append({
                    'agent': agent.agent_id,
                    'status': 'error',
                    'error': str(response)
                })
            else:
                # æå–ç­”æ¡ˆ
                answer = self._extract_answer(response)
                votes.append((answer, agent.weight))

                # æ”¶é›†è°ƒè¯•ä¿¡æ¯
                if debug:
                    debug_info.append({
                        'agent': agent.agent_id,
                        'raw_response': response[:300] + '...' if len(response) > 300 else response,
                        'extracted_answer': answer,
                        'weight': agent.weight
                    })

        # å¦‚æœå¯ç”¨è°ƒè¯•ï¼Œæ‰“å°æå–è¿‡ç¨‹
        if debug and debug_info:
            print("\n" + "=" * 60)
            print("DEBUG: Answer Extraction Process")
            print("=" * 60)
            for info in debug_info:
                if info.get('status') != 'error':
                    print(f"\n{info['agent']}:")
                    print(f"  Raw Response: {info['raw_response']}")
                    print(f"  Extracted: {info['extracted_answer']}")
                    print(f"  Weight: {info['weight']}")
            print("=" * 60 + "\n")

        # åŠ æƒæŠ•ç¥¨
        final_answer, voting_details = self._weighted_vote(votes)

        if debug_info:
            voting_details['debug_info'] = debug_info

        return final_answer, voting_details

    def _extract_answer(self, response: str) -> str:
        """
        ä»å›å¤ä¸­æå–ç­”æ¡ˆå­—æ¯ï¼ˆé²æ£’ç‰ˆæœ¬ï¼‰

        ç­–ç•¥ä¼˜å…ˆçº§ï¼š
        1. **Answer: X** æ ¼å¼ï¼ˆæœ€å¯é ï¼‰
        2. æœ€åä¸€ä¸ªå‡ºç°çš„ç‹¬ç«‹å­—æ¯ï¼ˆA/B/C/Dï¼‰
        3. <think>æ ‡ç­¾åçš„ç¬¬ä¸€ä¸ªå­—æ¯
        4. æ•´ä¸ªæ–‡æœ¬ä¸­ç¬¬ä¸€ä¸ªå‡ºç°çš„å­—æ¯
        """
        import re

        # ç­–ç•¥1ï¼šæŸ¥æ‰¾ **Answer: X** æ ¼å¼ï¼ˆæœ€ä¼˜å…ˆï¼‰
        answer_pattern = r'\*\*Answer:\s*([A-D])\*\*'
        match = re.search(answer_pattern, response, re.IGNORECASE)
        if match:
            return match.group(1).upper()

        # ç­–ç•¥2ï¼šæŸ¥æ‰¾ Answer: X æ ¼å¼ï¼ˆæ— æ˜Ÿå·ï¼‰
        answer_pattern_simple = r'(?:Answer|ç­”æ¡ˆ):\s*([A-D])'
        match = re.search(answer_pattern_simple, response, re.IGNORECASE)
        if match:
            return match.group(1).upper()

        # ç­–ç•¥3ï¼šæå– </think> æ ‡ç­¾ä¹‹åçš„å†…å®¹
        think_split = response.split('</think>')
        if len(think_split) > 1:
            after_think = think_split[-1]  # å–æœ€åä¸€ä¸ª </think> ä¹‹åçš„å†…å®¹

            # åœ¨ </think> åæŸ¥æ‰¾ç‹¬ç«‹çš„å­—æ¯
            # åŒ¹é…æ¨¡å¼ï¼šè¡Œé¦–ã€ç©ºæ ¼ã€æ ‡ç‚¹åçš„å•ç‹¬å­—æ¯
            letter_pattern = r'(?:^|\s|[.!?\n])\s*([A-D])(?:\s|[.!?,\n]|$)'
            match = re.search(letter_pattern, after_think, re.MULTILINE | re.IGNORECASE)
            if match:
                return match.group(1).upper()

            # å¦‚æœæ²¡æ‰¾åˆ°ç‹¬ç«‹å­—æ¯ï¼Œæ‰¾ç¬¬ä¸€ä¸ªå­—æ¯
            for char in after_think:
                if char.upper() in ['A', 'B', 'C', 'D']:
                    return char.upper()

        # ç­–ç•¥4ï¼šæŸ¥æ‰¾æœ€åä¸€ä¸ªå‡ºç°çš„ç‹¬ç«‹å­—æ¯ï¼ˆå¯èƒ½æ˜¯æ€»ç»“æ—¶çš„ç­”æ¡ˆï¼‰
        lines = response.strip().split('\n')
        for line in reversed(lines):
            line = line.strip()
            # æ£€æŸ¥æ˜¯å¦æ˜¯å•ç‹¬çš„å­—æ¯è¡Œ
            if len(line) == 1 and line.upper() in ['A', 'B', 'C', 'D']:
                return line.upper()
            # æ£€æŸ¥æ˜¯å¦åŒ…å« "é€‰X" æˆ– "choose X" ç­‰æ¨¡å¼
            choice_pattern = r'(?:é€‰æ‹©?|choose|select|pick)\s*([A-D])'
            match = re.search(choice_pattern, line, re.IGNORECASE)
            if match:
                return match.group(1).upper()

        # ç­–ç•¥5ï¼šæŸ¥æ‰¾ "X is correct" æˆ– "X æ˜¯æ­£ç¡®çš„" æ¨¡å¼
        correct_pattern = r'([A-D])\s*(?:is|ä¸º|æ˜¯)\s*(?:correct|right|æ­£ç¡®)'
        matches = re.findall(correct_pattern, response, re.IGNORECASE)
        if matches:
            return matches[-1].upper()  # å–æœ€åä¸€ä¸ªåŒ¹é…

        # ç­–ç•¥6ï¼šæŸ¥æ‰¾æ‰€æœ‰ç‹¬ç«‹å‡ºç°çš„å­—æ¯ï¼Œå–æœ€åä¸€ä¸ª
        all_letters = re.findall(r'(?:^|\s|[.!?\n])\s*([A-D])(?:\s|[.!?,\n]|$)', response,
                                 re.MULTILINE | re.IGNORECASE)
        if all_letters:
            return all_letters[-1].upper()

        # ç­–ç•¥7ï¼šåœ¨æ•´ä¸ªæ–‡æœ¬ä¸­æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå­—æ¯ï¼ˆæœ€åçš„å…œåº•ï¼‰
        for char in response:
            if char.upper() in ['A', 'B', 'C', 'D']:
                return char.upper()

        # å¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œè¿”å›INVALID
        return "INVALID"

    def _weighted_vote(self, votes: List[Tuple[str, float]]) -> Tuple[str, Dict]:
        """
        åŠ æƒæŠ•ç¥¨æœºåˆ¶

        Args:
            votes: List of (answer, weight)

        Returns:
            (final_answer, details)
        """
        # ç»Ÿè®¡æ¯ä¸ªç­”æ¡ˆçš„åŠ æƒå¾—åˆ†
        scores = {}
        for answer, weight in votes:
            if answer not in scores:
                scores[answer] = 0.0
            scores[answer] += weight

        # æ‰¾å‡ºå¾—åˆ†æœ€é«˜çš„ç­”æ¡ˆ
        if not scores:
            final_answer = "INVALID"
        else:
            final_answer = max(scores.items(), key=lambda x: x[1])[0]

        # æ„å»ºè¯¦ç»†ä¿¡æ¯
        details = {
            'votes': votes,
            'scores': scores,
            'final_answer': final_answer
        }

        return final_answer, details


class VotingMetrics:
    """æŠ•ç¥¨ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""

    def __init__(self):
        self.correct = 0
        self.total = 0
        self.total_time = 0.0

        # æ¯ä¸ªæ™ºèƒ½ä½“çš„å‡†ç¡®ç‡
        self.agent_correct = {}
        self.agent_total = {}

        # æŠ•ç¥¨ç»Ÿè®¡
        self.unanimous_votes = 0  # ä¸€è‡´æŠ•ç¥¨
        self.split_votes = 0  # åˆ†æ­§æŠ•ç¥¨

        # è¯¦ç»†è®°å½•
        self.results = []

    def update(self,
               predicted: str,
               target: str,
               voting_details: Dict,
               question: str,
               execution_time: float):
        """æ›´æ–°æŒ‡æ ‡"""
        is_correct = (predicted == target)
        self.correct += int(is_correct)
        self.total += 1
        self.total_time += execution_time

        # è®°å½•æ¯ä¸ªæ™ºèƒ½ä½“çš„è¡¨ç°
        for answer, weight in voting_details['votes']:
            agent_id = f"agent_{voting_details['votes'].index((answer, weight))}"

            if agent_id not in self.agent_correct:
                self.agent_correct[agent_id] = 0
                self.agent_total[agent_id] = 0

            self.agent_total[agent_id] += 1
            if answer == target:
                self.agent_correct[agent_id] += 1

        # æŠ•ç¥¨ä¸€è‡´æ€§åˆ†æ
        answers = [vote[0] for vote in voting_details['votes']]
        if len(set(answers)) == 1:
            self.unanimous_votes += 1
        else:
            self.split_votes += 1

        # è¯¦ç»†è®°å½•
        self.results.append({
            'question': question[:100] + '...',
            'predicted': predicted,
            'target': target,
            'correct': is_correct,
            'votes': voting_details['votes'],
            'scores': voting_details['scores'],
            'time': execution_time
        })

    def get_accuracy(self) -> float:
        return self.correct / self.total if self.total > 0 else 0.0

    def get_agent_accuracy(self, agent_id: str) -> float:
        if agent_id not in self.agent_total or self.agent_total[agent_id] == 0:
            return 0.0
        return self.agent_correct[agent_id] / self.agent_total[agent_id]

    def print_summary(self):
        print("\n" + "=" * 80)
        print("VOTING SYSTEM PERFORMANCE SUMMARY")
        print("=" * 80)

        print(f"\nğŸ“Š Overall Performance:")
        print(f"  Accuracy: {self.get_accuracy():.2%} ({self.correct}/{self.total})")
        print(f"  Avg Time: {self.total_time / self.total:.2f}s per question")

        print(f"\nğŸ¤– Individual Agent Performance:")
        for agent_id in sorted(self.agent_correct.keys()):
            acc = self.get_agent_accuracy(agent_id)
            correct = self.agent_correct[agent_id]
            total = self.agent_total[agent_id]
            print(f"  {agent_id}: {acc:.2%} ({correct}/{total})")

        print(f"\nğŸ—³ï¸  Voting Statistics:")
        print(f"  Unanimous Votes: {self.unanimous_votes} ({self.unanimous_votes / self.total:.1%})")
        print(f"  Split Votes: {self.split_votes} ({self.split_votes / self.total:.1%})")

        print(f"\nğŸ’° Cost Estimate:")
        print(f"  Total Cost: ${Cost.instance().value:.4f}")
        print(f"  Prompt Tokens: {PromptTokens.instance().value / 1000:.1f}k")
        print(f"  Completion Tokens: {CompletionTokens.instance().value / 1000:.1f}k")

        print("\n" + "=" * 80)

    def save_results(self, output_path: Path):
        """ä¿å­˜è¯¦ç»†ç»“æœ"""
        results_dict = {
            'summary': {
                'accuracy': self.get_accuracy(),
                'correct': self.correct,
                'total': self.total,
                'avg_time': self.total_time / self.total if self.total > 0 else 0,
                'unanimous_votes': self.unanimous_votes,
                'split_votes': self.split_votes,
                'total_cost': Cost.instance().value
            },
            'agent_performance': {
                agent_id: {
                    'accuracy': self.get_agent_accuracy(agent_id),
                    'correct': self.agent_correct[agent_id],
                    'total': self.agent_total[agent_id]
                }
                for agent_id in self.agent_correct.keys()
            },
            'results': self.results
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, indent=2, ensure_ascii=False)

        print(f"\nâœ“ Results saved to: {output_path}")


async def run_voting_experiment(
        num_agents: int,
        weights: List[float] = None,
        limit_questions: int = None,
        batch_size: int = 4,
        save_results: bool = True,
        debug_first_n: int = 3,  # å‰Nä¸ªé—®é¢˜å¯ç”¨è°ƒè¯•
        **kwargs
):
    """
    è¿è¡Œå¤šLLMæŠ•ç¥¨å®éªŒ

    Args:
        num_agents: æ™ºèƒ½ä½“æ•°é‡ï¼ˆ3æˆ–6ï¼‰
        weights: è‡ªå®šä¹‰æƒé‡åˆ—è¡¨
        limit_questions: é™åˆ¶é—®é¢˜æ•°é‡
        batch_size: æ‰¹å¤„ç†å¤§å°
        save_results: æ˜¯å¦ä¿å­˜ç»“æœ
        debug_first_n: å‰Nä¸ªé—®é¢˜å¯ç”¨è°ƒè¯•æ¨¡å¼
    """

    # é…ç½®LLMå’Œæƒé‡
    llm_configs = get_llm_configs(num_agents, weights)

    # åˆå§‹åŒ–æŠ•ç¥¨ç³»ç»Ÿ
    voting_system = MultiLLMVotingSystem(llm_configs, domain="mmlu")

    # åŠ è½½æ•°æ®é›†
    print("Loading MMLU validation dataset...")
    download()
    dataset = MMLUDataset('val')

    total_questions = min(len(dataset), limit_questions) if limit_questions else len(dataset)
    print(f"Testing on {total_questions} questions")
    if debug_first_n > 0:
        print(f"Debug mode enabled for first {debug_first_n} questions\n")
    else:
        print()

    # åˆå§‹åŒ–æŒ‡æ ‡
    metrics = VotingMetrics()

    # é‡ç½®è®¡æ•°å™¨
    Cost.instance().reset()
    PromptTokens.instance().reset()
    CompletionTokens.instance().reset()

    # æ‰¹å¤„ç†æ‰§è¡Œ
    num_batches = math.ceil(total_questions / batch_size)

    for batch_idx in tqdm(range(num_batches), desc="Processing batches"):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_questions)

        batch_tasks = []
        batch_records = []

        for idx in range(start_idx, end_idx):
            record = dataset[idx]
            input_dict = dataset.record_to_input(record)
            question = input_dict['task']

            # å‰Nä¸ªé—®é¢˜å¯ç”¨è°ƒè¯•
            enable_debug = (idx < debug_first_n)

            batch_tasks.append(voting_system.vote_on_question(question, debug=enable_debug))
            batch_records.append(record)

        # å¹¶å‘æ‰§è¡Œæ‰¹æ¬¡
        batch_start = time.time()
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
        batch_time = time.time() - batch_start

        # å¤„ç†ç»“æœ
        for record, result in zip(batch_records, batch_results):
            if isinstance(result, Exception):
                print(f"\nâŒ Error: {result}")
                continue

            final_answer, voting_details = result
            target = dataset.record_to_target_answer(record)
            question = dataset.record_to_input(record)['task']

            # æ›´æ–°æŒ‡æ ‡
            metrics.update(
                predicted=final_answer,
                target=target,
                voting_details=voting_details,
                question=question,
                execution_time=batch_time / len(batch_records)
            )

        # æ¯5ä¸ªæ‰¹æ¬¡æ‰“å°è¿›åº¦
        if (batch_idx + 1) % 5 == 0:
            print(f"\n--- Progress: {end_idx}/{total_questions} ---")
            print(f"  Current Accuracy: {metrics.get_accuracy():.2%}")
            print(f"  Avg Time: {metrics.total_time / metrics.total:.2f}s")

    # æ‰“å°æœ€ç»ˆç»“æœ
    metrics.print_summary()

    # ä¿å­˜ç»“æœ
    if save_results:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        result_dir = GDesigner_ROOT / "result" / "multi_llm_voting"
        result_dir.mkdir(parents=True, exist_ok=True)

        output_file = result_dir / f"voting_{num_agents}agents_{timestamp}.json"
        metrics.save_results(output_file)

    # WandBè®°å½•
    if "wandb_run" in kwargs:
        kwargs["wandb_run"].log({
            "accuracy": metrics.get_accuracy(),
            "unanimous_votes_ratio": metrics.unanimous_votes / metrics.total,
            "avg_time": metrics.total_time / metrics.total,
            "total_cost": Cost.instance().value
        })

        # è®°å½•æ¯ä¸ªagentçš„å‡†ç¡®ç‡
        for agent_id in metrics.agent_correct.keys():
            kwargs["wandb_run"].log({
                f"agent_accuracy/{agent_id}": metrics.get_agent_accuracy(agent_id)
            })

    return metrics


def get_llm_configs(num_agents: int, weights: List[float] = None) -> List[Tuple[str, float]]:
    """
    è·å–LLMé…ç½®å’Œæƒé‡

    Args:
        num_agents: æ™ºèƒ½ä½“æ•°é‡ï¼ˆ3æˆ–6ï¼‰
        weights: è‡ªå®šä¹‰æƒé‡åˆ—è¡¨

    Returns:
        List of (llm_name, weight)
    """
    # å¯ç”¨çš„LLMæ¨¡å‹ï¼ˆæŒ‰è§„æ¨¡ä»å°åˆ°å¤§ï¼‰
    available_models = [
        "Qwen/Qwen3-0.6B",
        "Qwen/Qwen3-1.7B",
        "Qwen/Qwen3-4B"
    ]

    # æ ¹æ®æ™ºèƒ½ä½“æ•°é‡é€‰æ‹©æ¨¡å‹
    if num_agents == 3:
        selected_models = available_models
    elif num_agents == 6:
        # æ¯ç§æ¨¡å‹å„ç”¨ä¸¤æ¬¡
        selected_models = [model for model in available_models for _ in range(2)]
    else:
        raise ValueError(f"Unsupported num_agents: {num_agents}. Only 3 or 6 are supported.")

    # è®¾ç½®æƒé‡
    if weights is None:
        # é»˜è®¤æƒé‡ï¼šæŒ‰æ¨¡å‹è§„æ¨¡é€’å¢
        if num_agents == 3:
            # weights = [0.22, 0.3, 0.48]  # å°æ¨¡å‹æƒé‡ä½ï¼Œå¤§æ¨¡å‹æƒé‡é«˜
            weights = [0.5274390243902439, 0.6185567010309279, 0.7941176470588235]  # å°æ¨¡å‹æƒé‡ä½ï¼Œå¤§æ¨¡å‹æƒé‡é«˜ 1.940113372479995
        elif num_agents == 6:
            # weights = [0.11, 0.11, 0.15, 0.15, 0.24, 0.24]
            weights = [0.5274390243902439, 0.5274390243902439, 0.6185567010309279, 0.6185567010309279,
                       0.7941176470588235, 0.7941176470588235]
    else:
        if len(weights) != num_agents:
            raise ValueError(f"Length of weights ({len(weights)}) must equal num_agents ({num_agents})")

    # æ ‡å‡†åŒ–æƒé‡
    total_weight = sum(weights)
    weights = [w / total_weight for w in weights]

    return list(zip(selected_models, weights))


def parse_args():
    parser = argparse.ArgumentParser(
        description="Multi-LLM Weighted Voting System for MMLU",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # 3ä¸ªæ™ºèƒ½ä½“ï¼Œé»˜è®¤æƒé‡
  python experiments/run_multi_llm_voting.py --num_agents 3 --limit 100

  # 6ä¸ªæ™ºèƒ½ä½“ï¼Œè‡ªå®šä¹‰æƒé‡
  python experiments/run_multi_llm_voting.py --num_agents 6 --weights 0.1 0.15 0.2 0.1 0.15 0.3

  # å®Œæ•´éªŒè¯é›†ï¼Œæ— é™åˆ¶
  python experiments/run_multi_llm_voting.py --num_agents 3
        """
    )

    parser.add_argument(
        '--num_agents',
        type=int,
        default=3,
        choices=[3, 6],
        help='Number of agents (3 or 6)'
    )

    parser.add_argument(
        '--weights',
        nargs='+',
        type=float,
        default=None,
        help='Custom weights for each agent (must sum close to 1.0)'
    )

    parser.add_argument(
        '--limit',
        type=int,
        default=153,
        help='Limit number of questions'
    )

    parser.add_argument(
        '--batch_size',
        type=int,
        default=8,
        help='Batch size for parallel processing'
    )

    parser.add_argument(
        '--debug_first_n',
        type=int,
        default=3,
        help='Enable debug mode for first N questions (default: 3)'
    )

    parser.add_argument(
        '--no_save',
        action='store_true',
        help='Do not save results'
    )

    parser.add_argument(
        '--weave_project',
        type=str,
        default='vito_chan/Multi-LLM-Voting',
        help='Weave project name'
    )

    return parser.parse_args()


async def main():
    import wandb

    args = parse_args()

    # åˆå§‹åŒ–è¿½è¸ª
    weave.init(project_name=args.weave_project)
    wandb_run = wandb.init(
        project="Multi-LLM-Voting",
        config=args,
        name=time.strftime("%Y-%m-%d_%H-%M-%S")
    )

    print("\n" + "=" * 80)
    print("MULTI-LLM WEIGHTED VOTING SYSTEM - MMLU EXPERIMENT")
    print("=" * 80)
    print(f"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Agents: {args.num_agents}")
    print(f"Custom Weights: {args.weights if args.weights else 'Default'}")
    print("=" * 80)

    try:
        metrics = await run_voting_experiment(
            num_agents=args.num_agents,
            weights=args.weights,
            limit_questions=args.limit,
            batch_size=args.batch_size,
            save_results=not args.no_save,
            debug_first_n=args.debug_first_n,
            wandb_run=wandb_run
        )

        print("\n" + "=" * 80)
        print("âœ… EXPERIMENT COMPLETED SUCCESSFULLY")
        print("=" * 80 + "\n")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  Experiment interrupted by user")
    except Exception as e:
        print(f"\n\nâŒ Experiment failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
