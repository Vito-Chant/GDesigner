"""
CoRe Graph Belief Training on MMLU Dev Set
åœ¨ MMLU dev é›†ä¸Šè®­ç»ƒ Belief Evolverï¼Œç„¶ååœ¨ val é›†ä¸Šæµ‹è¯•

Usage:
    python experiments/train_belief_mmlu.py --llm_name "your-model" --train_samples 100
"""

import sys
import os

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
sys.stdout.reconfigure(encoding='utf-8')

import asyncio
import argparse
import time
import json
from pathlib import Path
from typing import Dict, List
from tqdm import tqdm
import math

import weave

# å¯¼å…¥ CoRe ç»„ä»¶
from GDesigner.CoRe.core_graph import CoReGraph, CoReResult
from GDesigner.CoRe.belief_evolver import InteractionTrace, BeliefUpdate
from GDesigner.utils.globals import Cost, PromptTokens, CompletionTokens
from GDesigner.utils.const import GDesigner_ROOT

# å¯¼å…¥ MMLU æ•°æ®é›†
from dataset.mmlu_dataset import MMLUDataset
from dataset.MMLU.download import download


class BeliefTrainingMetrics:
    """ä¿¡å¿µè®­ç»ƒæŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self):
        self.total_samples = 0
        self.correct_samples = 0
        self.total_belief_updates = 0
        self.positive_updates = 0
        self.negative_updates = 0

        # æŒ‰æ›´æ–°ç±»å‹åˆ†ç±»
        self.update_types = {
            'positive_reinforcement': 0,
            'failure_attribution': 0,
            'nuanced_update': 0
        }

        # ä¿¡å¿µå˜åŒ–è®°å½•
        self.belief_changes = []

    def update(self, is_correct: bool, belief_updates: List[BeliefUpdate], result: CoReResult):
        """æ›´æ–°è®­ç»ƒæŒ‡æ ‡"""
        self.total_samples += 1
        if is_correct:
            self.correct_samples += 1

        self.total_belief_updates += len(belief_updates)

        for update in belief_updates:
            # ç»Ÿè®¡æ­£è´Ÿæ›´æ–°
            if update.confidence_change > 0:
                self.positive_updates += 1
            elif update.confidence_change < 0:
                self.negative_updates += 1

            # è®°å½•å˜åŒ–
            self.belief_changes.append({
                'from': update.from_agent,
                'to': update.to_agent,
                'old_belief': update.old_belief,
                'new_belief': update.new_belief,
                'confidence_change': update.confidence_change,
                'reason': update.update_reason
            })

    def get_accuracy(self) -> float:
        return self.correct_samples / self.total_samples if self.total_samples > 0 else 0.0

    def print_summary(self):
        print("\n" + "=" * 80)
        print("BELIEF TRAINING SUMMARY")
        print("=" * 80)

        print(f"\nğŸ“Š Training Metrics:")
        print(f"  Samples Processed: {self.total_samples}")
        print(f"  Accuracy: {self.get_accuracy():.2%} ({self.correct_samples}/{self.total_samples})")

        print(f"\nğŸ§  Belief Updates:")
        print(f"  Total Updates: {self.total_belief_updates}")
        print(f"  Positive (â†‘): {self.positive_updates}")
        print(f"  Negative (â†“): {self.negative_updates}")
        print(f"  Neutral (â†’): {self.total_belief_updates - self.positive_updates - self.negative_updates}")

        if self.belief_changes:
            print(f"\nğŸ“ˆ Recent Belief Changes (last 5):")
            for i, change in enumerate(self.belief_changes[-5:], 1):
                print(f"\n  {i}. {change['from']} â†’ {change['to']}")
                print(f"     New: {change['new_belief'][:80]}...")
                print(f"     Î” Confidence: {change['confidence_change']:+.2f}")
                print(f"     Reason: {change['reason'][:60]}...")


async def train_beliefs_on_mmlu_dev(
        llm_name: str,
        available_roles: List[str],
        decision_method: str = "FinalRefer",
        num_rounds: int = 1,
        max_routing: int = 5,
        train_samples: int = 100,
        batch_size: int = 1,
        save_registry: bool = True,
        **kwargs
):
    """
    åœ¨ MMLU dev é›†ä¸Šè®­ç»ƒä¿¡å¿µç³»ç»Ÿ

    Args:
        llm_name: LLM æ¨¡å‹åç§°
        available_roles: å¯ç”¨çš„ Agent è§’è‰²åˆ—è¡¨
        decision_method: å†³ç­–æ–¹æ³•
        num_rounds: æ¯ä¸ª Agent çš„è½®æ•°
        max_routing: æœ€å¤§è·¯ç”±æ­¥æ•°
        train_samples: è®­ç»ƒæ ·æœ¬æ•°é‡
        batch_size: æ‰¹å¤„ç†å¤§å°
        save_registry: æ˜¯å¦ä¿å­˜ MindRegistry
    """

    print("\n" + "=" * 80)
    print("BELIEF TRAINING ON MMLU DEV SET")
    print("=" * 80)
    print(f"  LLM: {llm_name}")
    print(f"  Roles: {', '.join(available_roles)}")
    print(f"  Training Samples: {train_samples}")
    print(f"  Batch Size: {batch_size}")
    print("=" * 80 + "\n")

    # å‡†å¤‡ä¿å­˜è·¯å¾„
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    result_dir = GDesigner_ROOT / "result" / "belief_training"
    result_dir.mkdir(parents=True, exist_ok=True)

    registry_save_path = None
    if save_registry:
        registry_save_path = result_dir / f"mind_registry_{timestamp}.json"

    # åˆå§‹åŒ– CoRe Graph
    core_graph = CoReGraph(
        domain="mmlu",
        llm_name=llm_name,
        available_roles=available_roles,
        decision_method=decision_method,
        max_routing=max_routing,
        registry_save_path=registry_save_path,
        rag_top_k=3,
        max_loop_count=4
    )

    # åŠ è½½è®­ç»ƒæ•°æ®é›†
    print("Loading MMLU dev dataset...")
    download()
    train_dataset = MMLUDataset('dev')

    # é™åˆ¶è®­ç»ƒæ ·æœ¬æ•°é‡
    total_samples = min(len(train_dataset), train_samples)
    print(f"Training on {total_samples} samples from dev set\n")

    # åˆå§‹åŒ–æŒ‡æ ‡
    metrics = BeliefTrainingMetrics()

    # é‡ç½®è®¡æ•°å™¨
    Cost.instance().reset()
    PromptTokens.instance().reset()
    CompletionTokens.instance().reset()

    # æ‰¹å¤„ç†è®­ç»ƒ
    num_batches = math.ceil(total_samples / batch_size)

    for batch_idx in tqdm(range(num_batches), desc="Training batches"):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_samples)

        batch_tasks = []
        batch_records = []

        for idx in range(start_idx, end_idx):
            record = train_dataset[idx]
            input_dict = train_dataset.record_to_input(record)

            batch_tasks.append(
                core_graph.run_cognitive_relay(input_dict)
            )
            batch_records.append(record)

        # å¹¶å‘æ‰§è¡Œæ‰¹æ¬¡
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

        # å¤„ç†ç»“æœå¹¶æ›´æ–°ä¿¡å¿µ
        for record, result in zip(batch_records, batch_results):
            if isinstance(result, Exception):
                print(f"\nâŒ Error: {result}")
                continue

            # è·å–ç­”æ¡ˆ
            predicted = train_dataset.postprocess_answer(result.final_answer)
            target = train_dataset.record_to_target_answer(record)
            is_correct = (predicted == target)

            # === å…³é”®æ­¥éª¤ï¼šä¿¡å¿µæ›´æ–° ===
            belief_updates = await process_execution_trace_and_update_beliefs(
                core_graph=core_graph,
                result=result,
                task_success=is_correct,
                question=train_dataset.record_to_input(record)['task']
            )

            # æ›´æ–°æŒ‡æ ‡
            metrics.update(is_correct, belief_updates, result)

        # æ¯ 10 ä¸ªæ‰¹æ¬¡æ‰“å°è¿›åº¦
        if (batch_idx + 1) % 10 == 0:
            print(f"\n--- Progress: {end_idx}/{total_samples} ---")
            print(f"  Current Accuracy: {metrics.get_accuracy():.2%}")
            print(f"  Total Belief Updates: {metrics.total_belief_updates}")
            print(f"  Avg Cost: ${Cost.instance().value:.4f}")

    # æ‰“å°è®­ç»ƒæ€»ç»“
    metrics.print_summary()

    print(f"\nğŸ’° Training Cost:")
    print(f"  Total: ${Cost.instance().value:.4f}")
    print(f"  Prompt Tokens: {PromptTokens.instance().value / 1000:.1f}k")
    print(f"  Completion Tokens: {CompletionTokens.instance().value / 1000:.1f}k")

    # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
    training_report = {
        'config': {
            'llm_name': llm_name,
            'available_roles': available_roles,
            'train_samples': total_samples,
            'batch_size': batch_size
        },
        'metrics': {
            'accuracy': metrics.get_accuracy(),
            'total_samples': metrics.total_samples,
            'correct_samples': metrics.correct_samples,
            'total_belief_updates': metrics.total_belief_updates,
            'positive_updates': metrics.positive_updates,
            'negative_updates': metrics.negative_updates
        },
        'belief_changes': metrics.belief_changes,
        'cost': {
            'total': Cost.instance().value,
            'prompt_tokens': PromptTokens.instance().value,
            'completion_tokens': CompletionTokens.instance().value
        }
    }

    report_path = result_dir / f"training_report_{timestamp}.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(training_report, f, indent=2, ensure_ascii=False)

    print(f"\nâœ“ Training report saved to: {report_path}")

    if save_registry:
        print(f"âœ“ Mind registry saved to: {registry_save_path}")

    if "wandb_run" in kwargs:
        kwargs["wandb_run"].log({
            "train/accuracy": metrics.get_accuracy(),
            "train/belief_updates": metrics.total_belief_updates,
            "train/positive_updates": metrics.positive_updates,
            "train/negative_updates": metrics.negative_updates,
            "train/cost": Cost.instance().value
        })

    return core_graph, registry_save_path


async def process_execution_trace_and_update_beliefs(
        core_graph: CoReGraph,
        result: CoReResult,
        task_success: bool,
        question: str
) -> List[BeliefUpdate]:
    """
    å¤„ç†æ‰§è¡Œè½¨è¿¹å¹¶æ›´æ–°ä¿¡å¿µ (v2 - ä¿®å¤ç‰ˆ)

    å…³é”®ä¿®å¤:
    1. æ­£ç¡®å¯¼å…¥ InteractionTrace
    2. å¤„ç† execution_trace ä¸­çš„ dict æ ¼å¼
    3. æ„å»ºå®Œæ•´çš„äº¤äº’é“¾
    """
    from GDesigner.CoRe.belief_evolver import InteractionTrace, BeliefUpdate

    all_updates = []

    # æ£€æŸ¥æ‰§è¡Œè½¨è¿¹æ˜¯å¦ä¸ºç©º
    if not result.execution_trace:
        print("[Warning] Empty execution trace, skipping belief update")
        return all_updates

    # éå†æ‰§è¡Œè½¨è¿¹ï¼ˆæ¯ä¸ª trace_step æ˜¯ä¸€ä¸ª dictï¼‰
    for i, trace_step in enumerate(result.execution_trace):
        try:
            # === 1. æå–å½“å‰æ­¥éª¤ä¿¡æ¯ ===
            current_agent = trace_step.get('agent', 'unknown')
            current_output = trace_step.get('output', '')

            # å¦‚æœ output æ˜¯ tupleï¼ˆæ¥è‡ª CoRe Agentï¼‰ï¼Œæå–å­—ç¬¦ä¸²éƒ¨åˆ†
            if isinstance(current_output, tuple):
                current_output = current_output[0] if len(current_output) > 0 else ''

            # === 2. æ‰¾åˆ°å¯¹åº”çš„è·¯ç”±å†³ç­– ===
            if i < len(result.routing_decisions):
                routing = result.routing_decisions[i]
                suggestion = routing.get('suggestion', 'Continue the work')
            else:
                suggestion = 'Complete the task'

            # === 3. ç¡®å®šä¸‹ä¸€ä¸ª Agent ===
            if i + 1 < len(result.execution_trace):
                next_step = result.execution_trace[i + 1]
                next_agent = next_step.get('agent', 'unknown')
            else:
                # æœ€åä¸€æ­¥ï¼Œä¸‹ä¸€ä¸ªæ˜¯ Decision Maker
                next_agent = core_graph.decision_maker_id

            # === 4. æ„å»º InteractionTrace å¯¹è±¡ ===
            interaction = InteractionTrace(
                from_agent=current_agent,
                to_agent=next_agent,
                task=question,
                suggestion=suggestion,
                output=current_output,
                success=task_success,  # æ•´ä½“ä»»åŠ¡æ˜¯å¦æˆåŠŸ
                failure_reason=None if task_success else "Task failed"
            )

            # === 5. æ„å»ºå®Œæ•´çš„äº¤äº’é“¾ï¼ˆç”¨äºä¸Šä¸‹æ–‡åˆ†æï¼‰===
            # BeliefEvolver éœ€è¦å®Œæ•´é“¾æ¥åšå¤±è´¥å½’å› 
            full_chain = []
            for j, step in enumerate(result.execution_trace):
                step_agent = step.get('agent', 'unknown')
                step_output = step.get('output', '')
                if isinstance(step_output, tuple):
                    step_output = step_output[0] if len(step_output) > 0 else ''

                # ç¡®å®šè¿™ä¸€æ­¥çš„ä¸‹ä¸€ä¸ª Agent
                if j + 1 < len(result.execution_trace):
                    step_next = result.execution_trace[j + 1].get('agent', 'unknown')
                else:
                    step_next = core_graph.decision_maker_id

                # æ„å»º InteractionTrace
                step_trace = InteractionTrace(
                    from_agent=step_agent,
                    to_agent=step_next,
                    task=question,
                    suggestion=result.routing_decisions[j].get('suggestion', '') if j < len(
                        result.routing_decisions) else '',
                    output=step_output,
                    success=(j == i) if not task_success else True  # åªæœ‰å¤±è´¥çš„é‚£ä¸€æ­¥æ ‡è®°ä¸ºå¤±è´¥
                )
                full_chain.append(step_trace)

            # === 6. è°ƒç”¨ BeliefEvolver æ›´æ–°ä¿¡å¿µ ===
            updates = await core_graph.belief_evolver.evolve_beliefs_from_interaction(
                interaction_trace=interaction,
                full_chain=full_chain,
                task_success=task_success,
                critic_feedback=None
            )

            all_updates.extend(updates)

        except Exception as e:
            # è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            import traceback
            print(f"[Warning] Belief update failed for {current_agent}:")
            print(f"  Error: {e}")
            print(f"  Trace step: {trace_step}")
            traceback.print_exc()
            continue

    return all_updates


async def test_with_trained_beliefs(
        core_graph: CoReGraph,
        test_samples: int = None,
        batch_size: int = 4,
        **kwargs
):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„ä¿¡å¿µåœ¨ val é›†ä¸Šæµ‹è¯•
    """

    print("\n" + "=" * 80)
    print("TESTING WITH TRAINED BELIEFS ON MMLU VAL SET")
    print("=" * 80)

    # åŠ è½½æµ‹è¯•æ•°æ®é›†
    test_dataset = MMLUDataset('val')
    total_samples = min(len(test_dataset), test_samples) if test_samples else len(test_dataset)

    print(f"Testing on {total_samples} samples from val set\n")

    # é‡ç½®è®¡æ•°å™¨
    Cost.instance().reset()
    PromptTokens.instance().reset()
    CompletionTokens.instance().reset()

    # æµ‹è¯•æŒ‡æ ‡
    correct = 0
    total = 0

    # æ‰¹å¤„ç†æµ‹è¯•
    num_batches = math.ceil(total_samples / batch_size)

    for batch_idx in tqdm(range(num_batches), desc="Testing batches"):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_samples)

        batch_tasks = []
        batch_records = []

        for idx in range(start_idx, end_idx):
            record = test_dataset[idx]
            input_dict = test_dataset.record_to_input(record)

            batch_tasks.append(
                core_graph.run_cognitive_relay(input_dict)
            )
            batch_records.append(record)

        # å¹¶å‘æ‰§è¡Œ
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

        # è®¡ç®—å‡†ç¡®ç‡
        for record, result in zip(batch_records, batch_results):
            if isinstance(result, Exception):
                continue

            predicted = test_dataset.postprocess_answer(result.final_answer)
            target = test_dataset.record_to_target_answer(record)

            if predicted == target:
                correct += 1
            total += 1

        # è¿›åº¦è¾“å‡º
        if (batch_idx + 1) % 10 == 0:
            accuracy = correct / total if total > 0 else 0
            print(f"\n--- Progress: {end_idx}/{total_samples} ---")
            print(f"  Current Accuracy: {accuracy:.2%}")

    # æœ€ç»ˆç»“æœ
    final_accuracy = correct / total if total > 0 else 0

    print("\n" + "=" * 80)
    print("TEST RESULTS")
    print("=" * 80)
    print(f"\nğŸ“Š Performance:")
    print(f"  Accuracy: {final_accuracy:.2%} ({correct}/{total})")
    print(f"\nğŸ’° Test Cost:")
    print(f"  Total: ${Cost.instance().value:.4f}")
    print(f"  Prompt Tokens: {PromptTokens.instance().value / 1000:.1f}k")
    print(f"  Completion Tokens: {CompletionTokens.instance().value / 1000:.1f}k")

    if "wandb_run" in kwargs:
        kwargs["wandb_run"].log({
            "test/accuracy": final_accuracy,
            "test/correct": correct,
            "test/total": total,
            "test/cost": Cost.instance().value
        })

    return final_accuracy


def parse_args():
    parser = argparse.ArgumentParser(
        description="Train CoRe Belief System on MMLU",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # è®­ç»ƒ 100 ä¸ªæ ·æœ¬
  python experiments/train_belief_mmlu.py --train_samples 100

  # è®­ç»ƒåç›´æ¥æµ‹è¯•
  python experiments/train_belief_mmlu.py --train_samples 200 --test_samples 500

  # è‡ªå®šä¹‰ Agent è§’è‰²
  python experiments/train_belief_mmlu.py --roles "Mathematician" "Critic" --train_samples 150
        """
    )

    # æ¨¡å‹é…ç½®
    parser.add_argument(
        '--llm_name',
        type=str,
        default="Qwen/Qwen3-4B-Instruct-2507",
        help='LLM model name'
    )

    # Agent é…ç½®
    parser.add_argument(
        '--roles',
        nargs='+',
        default=['Mathematician', 'Programmer', 'Knowlegable Expert', 'Critic'],
        help='List of agent roles'
    )

    parser.add_argument(
        '--decision_method',
        type=str,
        default='FinalRefer',
        help='Decision method'
    )

    # æ‰§è¡Œé…ç½®
    parser.add_argument(
        '--num_rounds',
        type=int,
        default=1,
        help='Number of rounds per agent'
    )

    parser.add_argument(
        '--max_routing',
        type=int,
        default=5,
        help='Maximum routing steps'
    )

    # è®­ç»ƒé…ç½®
    parser.add_argument(
        '--train_samples',
        type=int,
        default=100,
        help='Number of training samples from dev set'
    )

    parser.add_argument(
        '--batch_size',
        type=int,
        default=8,
        help='Batch size for training'
    )

    # æµ‹è¯•é…ç½®
    parser.add_argument(
        '--test_samples',
        type=int,
        default=None,
        help='Number of test samples from val set (None = all)'
    )

    parser.add_argument(
        '--test_batch_size',
        type=int,
        default=4,
        help='Batch size for testing'
    )

    # è¾“å‡ºé…ç½®
    parser.add_argument(
        '--no_save_registry',
        action='store_true',
        help='Do not save MindRegistry'
    )

    parser.add_argument(
        '--weave_project',
        type=str,
        default='vito_chan/CoRe-Belief-Training',
        help='Weave project name'
    )

    return parser.parse_args()


async def main():
    import wandb

    args = parse_args()

    # å¤„ç†è§’è‰²åç§°
    if len(args.roles) == 1 and ' ' in args.roles[0]:
        args.roles = args.roles[0].split()
    args.roles = [r.replace('_', ' ') for r in args.roles]

    # åˆå§‹åŒ–è¿½è¸ª
    weave.init(project_name=args.weave_project)
    wandb_run = wandb.init(
        project="CoRe-Belief-Training",
        config=args,
        name=time.strftime("%Y-%m-%d_%H-%M-%S")
    )

    print("\n" + "=" * 80)
    print("CoRe BELIEF TRAINING & TESTING PIPELINE")
    print("=" * 80)
    print(f"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    try:
        # ===== é˜¶æ®µ 1: è®­ç»ƒä¿¡å¿µ =====
        core_graph, registry_path = await train_beliefs_on_mmlu_dev(
            llm_name=args.llm_name,
            available_roles=args.roles,
            decision_method=args.decision_method,
            num_rounds=args.num_rounds,
            max_routing=args.max_routing,
            train_samples=args.train_samples,
            batch_size=args.batch_size,
            save_registry=not args.no_save_registry,
            wandb_run=wandb_run
        )

        print("\n" + "=" * 80)
        print("âœ… BELIEF TRAINING COMPLETED")
        print("=" * 80)

        # ===== é˜¶æ®µ 2: æµ‹è¯• =====
        if args.test_samples is not None or input("\nRun testing? (y/n): ").lower() == 'y':
            accuracy = await test_with_trained_beliefs(
                core_graph=core_graph,
                test_samples=args.test_samples,
                batch_size=args.test_batch_size,
                wandb_run=wandb_run
            )

            print("\n" + "=" * 80)
            print("âœ… TESTING COMPLETED")
            print(f"Final Accuracy: {accuracy:.2%}")
            print("=" * 80)

        print("\n" + "=" * 80)
        print("ğŸ‰ PIPELINE COMPLETED SUCCESSFULLY")
        print("=" * 80)

    except KeyboardInterrupt:
        print("\n\nâš ï¸  Pipeline interrupted by user")
    except Exception as e:
        print(f"\n\nâŒ Pipeline failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())