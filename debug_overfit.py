import os
import glob
import pickle
import numpy as np
import pandas as pd
import random
import warnings
import time
from tqdm import tqdm
from sklearn.metrics import roc_auc_score
from scipy.stats import skew, kurtosis
from concurrent.futures import ProcessPoolExecutor
from joblib import Parallel, delayed
import multiprocessing

# ============================
# é…ç½®åŒºåŸŸ
# ============================
VAL_DIR = "validation_data_v2"  # ç¡®ä¿æŒ‡å‘åŒ…å« batch_x.pkl çš„ç›®å½•

# æŒ–æ˜ç­–ç•¥é…ç½®
N_SESSIONS = 1000  # è¿è¡Œè½®æ•°
MAX_GENS_PER_SESSION = 200
PATIENCE = 15  # æ—©åœè€å¿ƒå€¼
POPULATION_SIZE = 500  # ç§ç¾¤å¤§å°
TOURNAMENT_SIZE = 8
N_JOBS = -1  # å¹¶è¡Œæ ¸å¿ƒæ•°

# ============================
# 0. é¡¶å±‚ç®—å­å®šä¹‰
# ============================
warnings.filterwarnings("ignore")


def op_add(x, y): return x + y


def op_sub(x, y): return x - y


def op_mul(x, y): return x * y


def op_div(x, y): return x / (y + 1e-6)


def op_abs(x): return np.abs(x)


def op_neg(x): return -x


def op_sq(x): return x ** 2


def op_sqrt(x): return np.sqrt(np.abs(x))


def op_max(x, y): return np.maximum(x, y)  # æ–°å¢ç®—å­


def op_min(x, y): return np.minimum(x, y)  # æ–°å¢ç®—å­


OPS = {
    'add': op_add, 'sub': op_sub, 'mul': op_mul, 'div': op_div,
    'abs': op_abs, 'neg': op_neg, 'sq': op_sq, 'sqrt': op_sqrt,
    'max': op_max, 'min': op_min
}

OP_METADATA = {
    'add': (2, "({} + {})"), 'sub': (2, "({} - {})"),
    'mul': (2, "({} * {})"), 'div': (2, "({} / {})"),
    'abs': (1, "abs({})"), 'neg': (1, "-({})"),
    'sq': (1, "({}**2)"), 'sqrt': (1, "sqrt(|{}|)"),
    'max': (2, "max({}, {})"), 'min': (2, "min({}, {})")
}


# ============================
# 1. å…¨æ™¯ç‰¹å¾æå– (Feature Engineering Level 1-4)
# ============================

def calc_slope(y):
    """ä¸€é˜¶è¶‹åŠ¿ (é€Ÿåº¦)"""
    n = len(y)
    if n < 2: return 0.0
    x = np.arange(n)
    x_mean = (n - 1) / 2.0
    y_mean = np.mean(y)
    numerator = np.sum((x - x_mean) * (y - y_mean))
    denominator = np.sum((x - x_mean) ** 2)
    return numerator / (denominator + 1e-9)


def calc_curvature(y):
    """äºŒé˜¶è¶‹åŠ¿ (åŠ é€Ÿåº¦): å‰åŠæ®µæ–œç‡ vs ååŠæ®µæ–œç‡"""
    n = len(y)
    if n < 4: return 0.0
    mid = n // 2
    slope1 = calc_slope(y[:mid])
    slope2 = calc_slope(y[mid:])
    return slope2 - slope1  # æ­£å€¼è¡¨ç¤ºåŠ é€Ÿä¸Šå‡ï¼Œè´Ÿå€¼è¡¨ç¤ºåŠ é€Ÿä¸‹é™


def get_quantiles(arr):
    """è¿”å› Q25, Q50(Median), Q75"""
    if len(arr) == 0: return 0, 0, 0
    return np.percentile(arr, [25, 50, 75])


def process_single_pkl(pkl_path):
    try:
        with open(pkl_path, "rb") as f:
            batch = pickle.load(f)
        results = []
        for item in batch:
            # === æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ ===
            feat = item.get('features')
            if feat is None or feat.shape[0] < 5: continue

            # è·å– GT Logprobs
            gt_logprobs = item.get('gt_logprobs')
            has_gt = (gt_logprobs is not None and len(gt_logprobs) == feat.shape[0])

            # åŸºç¡€åºåˆ—æå–
            logprobs = feat[:, 0]  # Top-1 Logprobs
            mask = logprobs > -99
            if not mask.any(): continue

            lp_vals = logprobs[mask]
            n_seq = len(lp_vals)
            prob_vals = np.exp(lp_vals)

            # Label
            row = {'label': float(item['is_correct'])}

            # ==========================================
            # Group A: æ ¸å¿ƒç½®ä¿¡åº¦ (Confidence)
            # ==========================================
            # 1. åŸºç¡€ç»Ÿè®¡
            row['lp_mean'] = np.mean(lp_vals)
            row['lp_std'] = np.std(lp_vals)
            row['lp_sum'] = np.sum(lp_vals)  # è”åˆæ¦‚ç‡

            # 2. åˆ†ä½æ•°ä¸ç¦»æ•£åº¦ (Robust Stats)
            q25, q50, q75 = get_quantiles(lp_vals)
            row['lp_q25'] = q25
            row['lp_median'] = q50
            row['lp_iqr'] = q75 - q25  # å››åˆ†ä½è·ï¼Œæ¯” std æ›´æŠ—å™ª

            # 3. æå€¼ä¸æœ¨æ¡¶æ•ˆåº”
            row['lp_min'] = np.min(lp_vals)
            # è®¡æ•°ç‰¹å¾: æœ‰å¤šå°‘ä¸ª token æå…¶ä¸è‡ªä¿¡ (-2.3 â‰ˆ 10% prob)
            row['lp_low_conf_count'] = np.sum(lp_vals < -2.3)
            row['lp_low_conf_ratio'] = row['lp_low_conf_count'] / n_seq

            # 4. æ—¶åºåŠ¨æ€
            row['lp_slope'] = calc_slope(lp_vals)  # çº¿æ€§è¶‹åŠ¿
            row['lp_curve'] = calc_curvature(lp_vals)  # åŠ é€Ÿåº¦
            row['lp_gap_fl'] = lp_vals[-1] - lp_vals[0]

            # 5. åˆ†æ®µç»Ÿè®¡ (å‰åŠæ®µ vs ååŠæ®µ)
            mid = n_seq // 2
            row['lp_mean_first'] = np.mean(lp_vals[:mid])
            row['lp_mean_last'] = np.mean(lp_vals[mid:])
            row['lp_ratio_fl'] = row['lp_mean_last'] / (row['lp_mean_first'] - 1e-9)  # é¿å…é™¤0

            # ==========================================
            # Group B: ä¸ç¡®å®šæ€§ä¸ç†µ (Entropy / Uncertainty)
            # ==========================================
            if feat.shape[1] > 1:
                probs_full = np.exp(feat[mask])
                p_sum = np.sum(probs_full, axis=1, keepdims=True) + 1e-10
                norm_p = probs_full / p_sum
                ents = -np.sum(norm_p * np.log(norm_p + 1e-10), axis=1)

                row['ent_mean'] = np.mean(ents)
                row['ent_std'] = np.std(ents)
                row['ent_max'] = np.max(ents)
                row['ent_slope'] = calc_slope(ents)

                # å˜å¼‚ç³»æ•° (Hesitation)
                row['prob_cv'] = np.std(prob_vals) / (np.mean(prob_vals) + 1e-9)

                # é«˜çº§äº¤äº’: ç†µä¸ç½®ä¿¡åº¦çš„èƒŒç¦»
                # ç†æƒ³æƒ…å†µ: ç½®ä¿¡åº¦é«˜(logprobå¤§)æ—¶ç†µåº”ä½ã€‚
                # å¦‚æœç›¸å…³æ€§å¼‚å¸¸ï¼Œè¯´æ˜æ¨¡å‹"ç›²ç›®è‡ªä¿¡"æˆ–"æ··ä¹±"
                if n_seq > 2:
                    # ç®€åŒ–ç‰ˆç›¸å…³æ€§ (Covariance)
                    cov = np.cov(lp_vals, ents)[0, 1]
                    row['cov_lp_ent'] = cov
                else:
                    row['cov_lp_ent'] = 0.0
            else:
                for c in ['ent_mean', 'ent_std', 'ent_max', 'ent_slope', 'prob_cv', 'cov_lp_ent']:
                    row[c] = 0.0

            # ==========================================
            # Group C: æƒŠè¯§åº¦ä¸çœŸå®æ€§ (Ground Truth / Surprise)
            # ==========================================
            if has_gt:
                gt_vals = gt_logprobs[mask]
                surprise_vals = lp_vals - gt_vals  # Top1 - GT (always >= 0)

                # GT åŸºç¡€ (Reading Comprehension)
                row['gt_mean'] = np.mean(gt_vals)
                row['gt_slope'] = calc_slope(gt_vals)

                # Surprise åŸºç¡€ (Alignment)
                row['surp_mean'] = np.mean(surprise_vals)
                row['surp_max'] = np.max(surprise_vals)
                row['surp_std'] = np.std(surprise_vals)

                # äº¤äº’ç‰¹å¾:
                # 1. æƒŠè¯§åº¦å æ¯”: é”™è¯¯æœ‰å¤šå°‘æ˜¯æ¥æºäº"æ„å¤–"?
                row['surp_ratio'] = np.sum(surprise_vals) / (np.abs(np.sum(lp_vals)) + 1e-9)

                # 2. ç»“å°¾æƒŠè¯§åº¦: ä¸´é—¨ä¸€è„šæ˜¯å¦å‡ºé”™?
                row['surp_last'] = surprise_vals[-1]

                # 3. æƒŠè¯§åº¦èšé›†æ€§: æœ€å¤§çš„æƒŠè¯§æ˜¯å¦å‘ç”Ÿåœ¨ååŠæ®µ?
                if n_seq > 1:
                    max_idx = np.argmax(surprise_vals)
                    row['surp_max_pos'] = max_idx / n_seq  # 0.0 ~ 1.0 (è¶Šæ¥è¿‘1è¶Šå±é™©)
                else:
                    row['surp_max_pos'] = 0.5
            else:
                for c in ['gt_mean', 'gt_slope', 'surp_mean', 'surp_max',
                          'surp_std', 'surp_ratio', 'surp_last', 'surp_max_pos']:
                    row[c] = 0.0

            results.append(row)
        return results
    except Exception as e:
        return []


def load_data_once(val_dir):
    print(f"æ­£åœ¨åŠ è½½æ•°æ®å¹¶è®¡ç®—å…¨æ™¯ç‰¹å¾ (v3) (CPU Cores: {multiprocessing.cpu_count()})...")
    if not os.path.exists(val_dir): return pd.DataFrame()
    pkl_files = glob.glob(os.path.join(val_dir, "*.pkl"))

    with ProcessPoolExecutor() as executor:
        results = list(tqdm(executor.map(process_single_pkl, pkl_files), total=len(pkl_files)))

    all_rows = [r for res in results for r in res]
    df = pd.DataFrame(all_rows)
    df.fillna(0, inplace=True)
    return df


# ============================
# 2. é—ä¼ è§„åˆ’é€»è¾‘ (GP Engine)
# ============================
class Individual:
    def __init__(self, expr_tree=None):
        self.expr_tree = expr_tree
        self.auc = 0.0
        self.formula_str = ""

    def __str__(self):
        return self.formula_str if self.formula_str else self._str_node(self.expr_tree)

    def _str_node(self, node):
        if isinstance(node, str): return node
        op_name = node[0]
        if op_name not in OP_METADATA: return "Error"
        fmt = OP_METADATA[op_name][1]
        args = [self._str_node(child) for child in node[1:]]
        return fmt.format(*args)


def evaluate_worker(expr_tree, df, y_true):
    try:
        def _eval(node, data):
            if isinstance(node, str):
                return data[node]
            elif isinstance(node, tuple):
                func = OPS[node[0]]
                args = [_eval(child, data) for child in node[1:]]
                return func(*args)
            return data.iloc[:, 0] * 0

        scores = _eval(expr_tree, df)
        scores = scores.replace([np.inf, -np.inf], np.nan).fillna(0)

        # æé€Ÿè®¡ç®— AUC (é¿å… sklearn å¼€é”€)
        # è¿™é‡Œè¿˜æ˜¯ç”¨ sklearn ä¿è¯å‡†ç¡®ï¼Œä½†å¦‚æœæ…¢å¯ä»¥æ¢ mannwhitneyu
        auc = roc_auc_score(y_true, scores)
        if auc < 0.5: auc = 1 - auc
        return auc
    except:
        return 0.0


def random_tree(features, depth=2):
    if depth == 0 or (depth < 2 and random.random() < 0.2):
        return random.choice(features)
    op = random.choice(list(OP_METADATA.keys()))
    children = [random_tree(features, depth - 1) for _ in range(OP_METADATA[op][0])]
    return (op, *children)


def mutate(individual, features):
    if random.random() < 0.4:
        return Individual(random_tree(features, depth=random.randint(1, 4)))
    return individual


def crossover(ind1, ind2):
    return ind1 if random.random() < 0.5 else ind2


# ============================
# 3. å•è½®ä¼šè¯é€»è¾‘
# ============================
def run_single_session(session_id, df, y, base_feats):
    new_seed = int(time.time()) + session_id * 1000
    random.seed(new_seed)
    np.random.seed(new_seed)

    print(f"\n>>> å¯åŠ¨ç¬¬ {session_id + 1}/{N_SESSIONS} è½®æŒ–æ˜ (Seed: {new_seed})")

    population = [Individual(random_tree(base_feats)) for _ in range(POPULATION_SIZE)]
    best_auc_this_session = 0.0
    no_improv_count = 0
    session_hall_of_fame = []

    for gen in range(MAX_GENS_PER_SESSION):
        trees = [ind.expr_tree for ind in population]
        aucs = Parallel(n_jobs=N_JOBS, prefer="processes")(
            delayed(evaluate_worker)(t, df, y) for t in trees
        )

        for i, ind in enumerate(population):
            ind.auc = aucs[i]
            ind.formula_str = ind._str_node(ind.expr_tree)

        valid_pop = [ind for ind in population if ind.auc > 0]
        valid_pop.sort(key=lambda x: x.auc, reverse=True)

        if not valid_pop:
            population = [Individual(random_tree(base_feats)) for _ in range(POPULATION_SIZE)]
            continue

        top_1 = valid_pop[0]

        if top_1.auc > best_auc_this_session + 0.0002:
            best_auc_this_session = top_1.auc
            no_improv_count = 0
            session_hall_of_fame.append(top_1)
        else:
            no_improv_count += 1

        if gen % 5 == 0:
            print(
                f"   [S{session_id + 1}] Gen {gen:02d} | Best AUC: {top_1.auc:.4f} | Patience: {no_improv_count}/{PATIENCE}")

        if no_improv_count >= PATIENCE:
            print(f"   ğŸ›‘ [æ—©åœ] æœ¬è½®åœ¨ {gen} ä»£ç»“æŸã€‚æœ€ä½³ AUC: {best_auc_this_session:.5f}")
            break

        next_gen = valid_pop[:5]
        while len(next_gen) < POPULATION_SIZE:
            parents = random.sample(valid_pop, min(len(valid_pop), TOURNAMENT_SIZE))
            p1 = max(parents, key=lambda x: x.auc)
            parents = random.sample(valid_pop, min(len(valid_pop), TOURNAMENT_SIZE))
            p2 = max(parents, key=lambda x: x.auc)
            next_gen.append(mutate(crossover(p1, p2), base_feats))
        population = next_gen

    return session_hall_of_fame


# ============================
# 4. ä¸»ç¨‹åº
# ============================
def main():
    # 1. å‡†å¤‡æ•°æ®
    df = load_data_once(VAL_DIR)
    if len(df) == 0: return print("æ— æ•°æ®ã€‚è¯·æ£€æŸ¥ç›®å½•ã€‚")
    y = df['label'].values
    base_feats = [c for c in df.columns if c != 'label']

    print(f"\nâœ… ç‰¹å¾æ± å‡çº§å®Œæˆï¼Œå…±åŒ…å« {len(base_feats)} ä¸ªå…¨æ™¯ç‰¹å¾ã€‚")
    print(f"ç¤ºä¾‹ç‰¹å¾: {random.sample(base_feats, min(5, len(base_feats)))}")

    # 2. å…¨å±€å®¹å™¨
    global_hall_of_fame = []

    # 3. å¾ªç¯è¿è¡Œ Session
    for i in range(N_SESSIONS):
        top_candidates = run_single_session(i, df, y, base_feats)
        global_hall_of_fame.extend(top_candidates)

        # æ¸…ç†ä¸å»é‡
        global_hall_of_fame.sort(key=lambda x: x.auc, reverse=True)
        unique_hof = []
        seen = set()
        for ind in global_hall_of_fame:
            if ind.formula_str not in seen:
                seen.add(ind.formula_str)
                unique_hof.append(ind)
        global_hall_of_fame = unique_hof[:50]

        if len(global_hall_of_fame) > 0:
            print(f"   >>> ğŸ† å…¨å±€æœ€ä½³: {global_hall_of_fame[0].auc:.5f} | {global_hall_of_fame[0].formula_str}")

    # 4. æœ€ç»ˆç»“æœ
    print("\n" + "=" * 80)
    print(f"ğŸ† æ‰€æœ‰æŒ–æ˜ç»“æŸï¼Top 15 è¶…çº§ç‰¹å¾")
    print("=" * 80)

    for i, ind in enumerate(global_hall_of_fame[:15]):
        print(f"Rank {i + 1:02d} | AUC: {ind.auc:.5f} | {ind.formula_str}")


if __name__ == "__main__":
    main()